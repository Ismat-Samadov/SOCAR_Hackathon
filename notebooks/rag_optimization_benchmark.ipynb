{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Optimization Benchmark\n",
    "\n",
    "**Comprehensive testing of ALL RAG components to maximize LLM Judge score**\n",
    "\n",
    "## What We're Testing:\n",
    "\n",
    "### 1. Embedding Models (Vector Representations)\n",
    "- `BAAI/bge-large-en-v1.5` (Current - 1024 dim, best quality)\n",
    "- `BAAI/bge-base-en-v1.5` (768 dim, faster)\n",
    "- `intfloat/multilingual-e5-large` (1024 dim, multi-language)\n",
    "- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (768 dim, multilingual)\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (384 dim, very fast)\n",
    "\n",
    "### 2. Retrieval Strategies\n",
    "- **Top-K**: Test 1, 3, 5, 10 documents\n",
    "- **MMR** (Maximal Marginal Relevance): Diversity vs relevance trade-off\n",
    "- **Similarity Threshold**: Filter low-relevance docs\n",
    "- **Reranking**: Use cross-encoder to rerank results\n",
    "\n",
    "### 3. Chunking Strategies (Already in Vector DB, but we'll compare)\n",
    "- Chunk size: 256, 512, 600 (current), 1000 tokens\n",
    "- Overlap: 0, 50, 100 (current), 200 chars\n",
    "\n",
    "### 4. LLM Models\n",
    "- Llama-4-Maverick-17B (open-source)\n",
    "- DeepSeek-R1 (reasoning)\n",
    "- GPT-4.1, GPT-5, GPT-5-mini\n",
    "- Claude-Sonnet-4.5\n",
    "\n",
    "### 5. Prompting Techniques\n",
    "- **Baseline**: Simple context + question\n",
    "- **Citation-focused**: Emphasize source references\n",
    "- **Step-by-step**: Chain-of-thought reasoning\n",
    "- **Few-shot**: Include example Q&A\n",
    "\n",
    "## LLM Judge Evaluation Criteria:\n",
    "- **Accuracy** (35%): Answer correctness\n",
    "- **Relevance** (35%): Citation quality and relevance\n",
    "- **Completeness** (30%): Thorough answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pinecone-client sentence-transformers rank-bm25 python-dotenv pandas matplotlib seaborn jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/SOCAR_Hackathon/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import AzureOpenAI\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Questions and Expected Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 5 test questions\n",
      "  - Example1\n",
      "  - Example2\n",
      "  - Example3\n",
      "  - Example4\n",
      "  - Example5\n"
     ]
    }
   ],
   "source": [
    "# Load test cases\n",
    "with open('docs/sample_questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open('docs/sample_answers.json', 'r', encoding='utf-8') as f:\n",
    "    expected_answers = json.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded {len(questions)} test questions\")\n",
    "for key in questions.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector DB connected\n",
      "   Total vectors: 1300\n",
      "   Dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "index = pc.Index(os.getenv('PINECONE_INDEX_NAME', 'hackathon'))\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"âœ… Vector DB connected\")\n",
    "print(f\"   Total vectors: {stats['total_vector_count']}\")\n",
    "print(f\"   Dimensions: {stats['dimension']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Models Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bge-large-en...\n",
      "  âœ… BAAI/bge-large-en-v1.5\n",
      "Loading multilingual-e5-large...\n",
      "  âœ… intfloat/multilingual-e5-large\n",
      "\n",
      "âœ… Loaded 2 embedding models\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODELS = {\n",
    "    'bge-large-en': {\n",
    "        'name': 'BAAI/bge-large-en-v1.5',\n",
    "        'dimensions': 1024,\n",
    "        'notes': 'Current model - best quality'\n",
    "    },\n",
    "    'bge-base-en': {\n",
    "        'name': 'BAAI/bge-base-en-v1.5',\n",
    "        'dimensions': 768,\n",
    "        'notes': 'Faster, slightly lower quality'\n",
    "    },\n",
    "    'multilingual-e5-large': {\n",
    "        'name': 'intfloat/multilingual-e5-large',\n",
    "        'dimensions': 1024,\n",
    "        'notes': 'Multi-language optimized'\n",
    "    },\n",
    "    'paraphrase-multilingual': {\n",
    "        'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "        'dimensions': 768,\n",
    "        'notes': 'Good for Azerbaijani/Russian'\n",
    "    },\n",
    "    'all-MiniLM-L6': {\n",
    "        'name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'dimensions': 384,\n",
    "        'notes': 'Very fast, lower quality'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load embedding models (only test 1024-dim models for existing Pinecone index)\n",
    "EMBEDDING_MODELS_TO_TEST = [\n",
    "    'bge-large-en',  # Current\n",
    "    'multilingual-e5-large',  # Alternative with same dims\n",
    "]\n",
    "\n",
    "embedding_cache = {}\n",
    "\n",
    "for model_key in EMBEDDING_MODELS_TO_TEST:\n",
    "    model_name = EMBEDDING_MODELS[model_key]['name']\n",
    "    print(f\"Loading {model_key}...\")\n",
    "    embedding_cache[model_key] = SentenceTransformer(model_name)\n",
    "    print(f\"  âœ… {model_name}\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(embedding_cache)} embedding models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured 7 retrieval strategies\n"
     ]
    }
   ],
   "source": [
    "def retrieve_vanilla(query: str, embed_model: SentenceTransformer, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Vanilla retrieval: Simple top-k vector search.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    documents = []\n",
    "    for match in results['matches']:\n",
    "        documents.append({\n",
    "            'pdf_name': match['metadata'].get('pdf_name', 'unknown.pdf'),\n",
    "            'page_number': match['metadata'].get('page_number', 0),\n",
    "            'content': match['metadata'].get('text', ''),\n",
    "            'score': match.get('score', 0.0)\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def retrieve_with_threshold(query: str, embed_model: SentenceTransformer, \n",
    "                           top_k: int = 10, threshold: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve with similarity threshold filtering.\n",
    "    \"\"\"\n",
    "    docs = retrieve_vanilla(query, embed_model, top_k=top_k)\n",
    "    return [doc for doc in docs if doc['score'] >= threshold]\n",
    "\n",
    "\n",
    "def retrieve_with_mmr(query: str, embed_model: SentenceTransformer, \n",
    "                     top_k: int = 3, lambda_param: float = 0.5, fetch_k: int = 20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    MMR (Maximal Marginal Relevance) for diversity.\n",
    "    lambda=1 â†’ pure relevance, lambda=0 â†’ pure diversity\n",
    "    \"\"\"\n",
    "    # Fetch more candidates\n",
    "    candidates = retrieve_vanilla(query, embed_model, top_k=fetch_k)\n",
    "    \n",
    "    if len(candidates) <= top_k:\n",
    "        return candidates[:top_k]\n",
    "    \n",
    "    # Query embedding\n",
    "    query_emb = embed_model.encode(query)\n",
    "    \n",
    "    # Get embeddings for candidates\n",
    "    candidate_texts = [doc['content'] for doc in candidates]\n",
    "    candidate_embs = embed_model.encode(candidate_texts)\n",
    "    \n",
    "    # MMR algorithm\n",
    "    selected = []\n",
    "    selected_embs = []\n",
    "    \n",
    "    for _ in range(min(top_k, len(candidates))):\n",
    "        mmr_scores = []\n",
    "        \n",
    "        for i, (doc, emb) in enumerate(zip(candidates, candidate_embs)):\n",
    "            if i in [candidates.index(s) for s in selected]:\n",
    "                mmr_scores.append(-float('inf'))\n",
    "                continue\n",
    "            \n",
    "            # Relevance to query\n",
    "            relevance = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n",
    "            \n",
    "            # Max similarity to already selected\n",
    "            if selected_embs:\n",
    "                similarities = [np.dot(emb, s_emb) / (np.linalg.norm(emb) * np.linalg.norm(s_emb)) \n",
    "                              for s_emb in selected_embs]\n",
    "                max_sim = max(similarities)\n",
    "            else:\n",
    "                max_sim = 0\n",
    "            \n",
    "            # MMR score\n",
    "            mmr = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "            mmr_scores.append(mmr)\n",
    "        \n",
    "        # Select best MMR score\n",
    "        best_idx = np.argmax(mmr_scores)\n",
    "        selected.append(candidates[best_idx])\n",
    "        selected_embs.append(candidate_embs[best_idx])\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "def retrieve_with_reranking(query: str, embed_model: SentenceTransformer, \n",
    "                           top_k: int = 3, fetch_k: int = 20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Two-stage: retrieve with embeddings, rerank with cross-encoder.\n",
    "    \"\"\"\n",
    "    # Stage 1: Retrieve candidates\n",
    "    candidates = retrieve_vanilla(query, embed_model, top_k=fetch_k)\n",
    "    \n",
    "    if len(candidates) <= top_k:\n",
    "        return candidates[:top_k]\n",
    "    \n",
    "    # Stage 2: Rerank with cross-encoder\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    \n",
    "    pairs = [[query, doc['content']] for doc in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort by reranker score\n",
    "    scored_docs = [(doc, score) for doc, score in zip(candidates, scores)]\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Update scores and return top-k\n",
    "    reranked = []\n",
    "    for doc, score in scored_docs[:top_k]:\n",
    "        doc['rerank_score'] = float(score)\n",
    "        reranked.append(doc)\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "\n",
    "RETRIEVAL_STRATEGIES = {\n",
    "    'vanilla_k3': {'func': retrieve_vanilla, 'params': {'top_k': 3}, 'notes': 'Current setup'},\n",
    "    'vanilla_k5': {'func': retrieve_vanilla, 'params': {'top_k': 5}, 'notes': 'More context'},\n",
    "    'vanilla_k10': {'func': retrieve_vanilla, 'params': {'top_k': 10}, 'notes': 'Maximum context'},\n",
    "    'threshold_0.7': {'func': retrieve_with_threshold, 'params': {'top_k': 10, 'threshold': 0.7}, 'notes': 'Quality filter'},\n",
    "    'mmr_balanced': {'func': retrieve_with_mmr, 'params': {'top_k': 3, 'lambda_param': 0.5}, 'notes': 'Balance diversity'},\n",
    "    'mmr_diverse': {'func': retrieve_with_mmr, 'params': {'top_k': 3, 'lambda_param': 0.3}, 'notes': 'More diversity'},\n",
    "    'reranked_k3': {'func': retrieve_with_reranking, 'params': {'top_k': 3, 'fetch_k': 20}, 'notes': 'Two-stage rerank'},\n",
    "}\n",
    "\n",
    "print(f\"âœ… Configured {len(RETRIEVAL_STRATEGIES)} retrieval strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Models and Prompting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured 5 LLM models\n",
      "âœ… Configured 4 prompting strategies\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    ")\n",
    "\n",
    "LLM_MODELS = {\n",
    "    'Llama-4-Maverick': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
    "    'DeepSeek-R1': 'DeepSeek-R1',\n",
    "    'GPT-4.1': 'gpt-4.1',\n",
    "    'GPT-5-mini': 'gpt-5-mini',\n",
    "    'Claude-Sonnet-4.5': 'claude-sonnet-4-5',\n",
    "}\n",
    "\n",
    "# Prompting strategies\n",
    "PROMPTING_STRATEGIES = {\n",
    "    'baseline': \"\"\"\n",
    "Siz SOCAR-Ä±n tarixi neft vÉ™ qaz sÉ™nÉ™dlÉ™ri Ã¼zrÉ™ kÃ¶mÉ™kÃ§isiniz.\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "KontekstÉ™ É™saslanaraq cavab verin.\n",
    "\"\"\",\n",
    "    \n",
    "    'citation_focused': \"\"\"\n",
    "Siz SOCAR-Ä±n tarixi sÉ™nÉ™dlÉ™r Ã¼zrÉ™ mÃ¼tÉ™xÉ™ssis kÃ¶mÉ™kÃ§isisiniz.\n",
    "\n",
    "Ã–NÆMLÄ°: HÉ™r bir faktÄ± mÃ¼tlÉ™q mÉ™nbÉ™ ilÉ™ tÉ™sdiqlÉ™yin (PDF adÄ± vÉ™ sÉ™hifÉ™ nÃ¶mrÉ™si).\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "Cavab verÉ™rkÉ™n:\n",
    "1. DÉ™qiq faktlar yazÄ±n\n",
    "2. HÉ™r faktÄ± mÉ™nbÉ™ ilÉ™ gÃ¶stÉ™rin: (PDF: fayl_adÄ±.pdf, SÉ™hifÉ™: X)\n",
    "3. KontekstdÉ™ olmayan mÉ™lumat É™lavÉ™ etmÉ™yin\n",
    "\"\"\",\n",
    "    \n",
    "    'step_by_step': \"\"\"\n",
    "Siz SOCAR-Ä±n tarixi sÉ™nÉ™dlÉ™r Ã¼zrÉ™ analitik kÃ¶mÉ™kÃ§isisiniz.\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "AddÄ±m-addÄ±m cavab verin:\n",
    "1. ÆvvÉ™lcÉ™ kontekstdÉ™n É™laqÉ™li mÉ™lumatlarÄ± mÃ¼É™yyÉ™nlÉ™ÅŸdirin\n",
    "2. Bu mÉ™lumatlarÄ± tÉ™hlil edin\n",
    "3. NÉ™ticÉ™ni mÉ™nbÉ™lÉ™r ilÉ™ birlikdÉ™ tÉ™qdim edin\n",
    "\"\"\",\n",
    "    \n",
    "    'few_shot': \"\"\"\n",
    "Siz SOCAR-Ä±n tarixi sÉ™nÉ™dlÉ™r Ã¼zrÉ™ mÃ¼tÉ™xÉ™ssis kÃ¶mÉ™kÃ§isisiniz.\n",
    "\n",
    "NÃ¼munÉ™:\n",
    "Sual: \"PalÃ§Ä±q vulkanlarÄ±nÄ±n tÉ™sir radiusu nÉ™ qÉ™dÉ™rdir?\"\n",
    "Cavab: \"SahÉ™ mÃ¼ÅŸahidÉ™lÉ™ri vÉ™ modellÉ™ÅŸdirmÉ™ gÃ¶stÉ™rir ki, palÃ§Ä±q vulkanlarÄ±nÄ±n tÉ™sir radiusu tÉ™qribÉ™n 10 km-dir (PDF: document_06.pdf, SÉ™hifÉ™: 5).\"\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "YuxarÄ±dakÄ± nÃ¼munÉ™ kimi cavab verin - dÉ™qiq, qÄ±sa, mÉ™nbÉ™ ilÉ™.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"âœ… Configured {len(LLM_MODELS)} LLM models\")\n",
    "print(f\"âœ… Configured {len(PROMPTING_STRATEGIES)} prompting strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_answer(llm_model: str, query: str, documents: List[Dict], \n                   prompt_strategy: str = 'baseline',\n                   temperature: float = 0.2) -> Tuple[str, float]:\n    \"\"\"\n    Generate answer using LLM with specified prompting strategy.\n    \"\"\"\n    # Build context\n    context_parts = []\n    for i, doc in enumerate(documents, 1):\n        context_parts.append(\n            f\"SÉ™nÉ™d {i} (MÉ™nbÉ™: {doc['pdf_name']}, SÉ™hifÉ™ {doc['page_number']}):\\n{doc['content']}\"\n        )\n    context = \"\\n\\n\".join(context_parts)\n    \n    # Get prompt template\n    prompt_template = PROMPTING_STRATEGIES[prompt_strategy]\n    prompt = prompt_template.format(context=context, query=query)\n    \n    try:\n        start_time = time.time()\n        \n        deployment = LLM_MODELS[llm_model]\n        \n        # GPT-5 models use max_completion_tokens, others use max_tokens\n        if deployment.startswith('gpt-5'):\n            response = azure_client.chat.completions.create(\n                model=deployment,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_completion_tokens=1000\n            )\n        else:\n            response = azure_client.chat.completions.create(\n                model=deployment,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=1000\n            )\n        \n        elapsed = time.time() - start_time\n        answer = response.choices[0].message.content\n        \n        return answer, elapsed\n    \n    except Exception as e:\n        return f\"ERROR: {str(e)}\", 0.0\n\nprint(\"âœ… LLM generation function ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation metrics ready\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_answer_quality(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Accuracy metrics.\"\"\"\n",
    "    ref_norm = normalize_text(reference)\n",
    "    hyp_norm = normalize_text(hypothesis)\n",
    "    \n",
    "    cer_score = cer(ref_norm, hyp_norm) * 100\n",
    "    wer_score = wer(ref_norm, hyp_norm) * 100\n",
    "    similarity = max(0, 100 - wer_score)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy_Score': round(similarity, 2)\n",
    "    }\n",
    "\n",
    "def evaluate_citation_quality(answer: str, documents: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Relevance - citation quality.\"\"\"\n",
    "    pdf_names = [doc['pdf_name'].replace('.pdf', '') for doc in documents]\n",
    "    page_numbers = [str(doc['page_number']) for doc in documents]\n",
    "    \n",
    "    cited_pdfs = sum(1 for pdf in pdf_names if pdf in answer)\n",
    "    cited_pages = sum(1 for page in page_numbers if page in answer)\n",
    "    \n",
    "    citation_keywords = ['mÉ™nbÉ™', 'sÉ™nÉ™d', 'sÉ™hifÉ™', 'pdf', 'document', 'page']\n",
    "    has_citation_format = any(kw in answer.lower() for kw in citation_keywords)\n",
    "    \n",
    "    citation_score = (\n",
    "        (cited_pdfs / len(pdf_names) * 40) +\n",
    "        (cited_pages / len(page_numbers) * 40) +\n",
    "        (20 if has_citation_format else 0)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Citation_Score': round(citation_score, 2),\n",
    "        'Cited_PDFs': cited_pdfs,\n",
    "        'Cited_Pages': cited_pages\n",
    "    }\n",
    "\n",
    "def evaluate_retrieval_quality(query: str, documents: List[Dict], expected_answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Measure if retrieved docs are relevant to answer.\"\"\"\n",
    "    if not documents or not expected_answer:\n",
    "        return {'Retrieval_Relevance': 0.0}\n",
    "    \n",
    "    # Simple heuristic: check if expected answer words appear in retrieved docs\n",
    "    expected_words = set(normalize_text(expected_answer).split())\n",
    "    retrieved_text = ' '.join([doc['content'] for doc in documents])\n",
    "    retrieved_words = set(normalize_text(retrieved_text).split())\n",
    "    \n",
    "    overlap = len(expected_words & retrieved_words) / len(expected_words) if expected_words else 0\n",
    "    \n",
    "    return {\n",
    "        'Retrieval_Relevance': round(overlap * 100, 2)\n",
    "    }\n",
    "\n",
    "def evaluate_completeness(answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Completeness metrics.\"\"\"\n",
    "    word_count = len(answer.split())\n",
    "    \n",
    "    if word_count < 20:\n",
    "        completeness = (word_count / 20) * 100\n",
    "    elif word_count > 200:\n",
    "        completeness = 100 - ((word_count - 200) / 200 * 20)\n",
    "    else:\n",
    "        completeness = 100\n",
    "    \n",
    "    return {\n",
    "        'Completeness_Score': round(max(0, completeness), 2),\n",
    "        'Word_Count': word_count\n",
    "    }\n",
    "\n",
    "def calculate_llm_judge_score(accuracy: float, citation: float, completeness: float) -> float:\n",
    "    \"\"\"Overall LLM Judge score (weighted).\"\"\"\n",
    "    return round(\n",
    "        accuracy * 0.35 +\n",
    "        citation * 0.35 +\n",
    "        completeness * 0.30,\n",
    "        2\n",
    "    )\n",
    "\n",
    "print(\"âœ… Evaluation metrics ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 11 configurations on 5 questions\n",
      "Total API calls: ~55\n",
      "This will take 15-30 minutes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Select what to test\n",
    "CONFIGS_TO_TEST = [\n",
    "    # Format: (embed_model, retrieval_strategy, llm_model, prompt_strategy)\n",
    "    \n",
    "    # Baseline (current setup)\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different embedding models\n",
    "    ('multilingual-e5-large', 'vanilla_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different retrieval strategies\n",
    "    ('bge-large-en', 'vanilla_k5', 'Llama-4-Maverick', 'baseline'),\n",
    "    ('bge-large-en', 'mmr_balanced', 'Llama-4-Maverick', 'baseline'),\n",
    "    ('bge-large-en', 'reranked_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different LLM models\n",
    "    ('bge-large-en', 'vanilla_k3', 'GPT-5-mini', 'baseline'),\n",
    "    ('bge-large-en', 'vanilla_k3', 'Claude-Sonnet-4.5', 'baseline'),\n",
    "    \n",
    "    # Test different prompting strategies\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'citation_focused'),\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'few_shot'),\n",
    "    \n",
    "    # Best combinations\n",
    "    ('bge-large-en', 'reranked_k3', 'GPT-5-mini', 'citation_focused'),\n",
    "    ('bge-large-en', 'mmr_balanced', 'Claude-Sonnet-4.5', 'citation_focused'),\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(CONFIGS_TO_TEST)} configurations on {len(questions)} questions\")\n",
    "print(f\"Total API calls: ~{len(CONFIGS_TO_TEST) * len(questions)}\")\n",
    "print(\"This will take 15-30 minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Config 1/11: bge-large-en_vanilla_k3_Llama-4-Maverick_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.01s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.38s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.45s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.52s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.14s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 43.53%\n",
      "     Avg Response Time: 2.50s\n",
      "\n",
      "====================================================================================================\n",
      "Config 2/11: multilingual-e5-large_vanilla_k3_Llama-4-Maverick_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.98s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.66s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.19s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 4.38s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 4.32s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 39.73%\n",
      "     Avg Response Time: 3.31s\n",
      "\n",
      "====================================================================================================\n",
      "Config 3/11: bge-large-en_vanilla_k5_Llama-4-Maverick_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 5 docs\n",
      "    âœ… Generated in 2.55s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 5 docs\n",
      "    âœ… Generated in 2.50s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 5 docs\n",
      "    âœ… Generated in 2.58s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 5 docs\n",
      "    âœ… Generated in 3.07s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 5 docs\n",
      "    âœ… Generated in 3.74s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 45.40%\n",
      "     Avg Response Time: 2.89s\n",
      "\n",
      "====================================================================================================\n",
      "Config 4/11: bge-large-en_mmr_balanced_Llama-4-Maverick_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.64s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.27s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.34s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.05s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.52s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 45.40%\n",
      "     Avg Response Time: 2.16s\n",
      "\n",
      "====================================================================================================\n",
      "Config 5/11: bge-large-en_reranked_k3_Llama-4-Maverick_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.26s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.12s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.83s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.93s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.24s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 44.47%\n",
      "     Avg Response Time: 3.08s\n",
      "\n",
      "====================================================================================================\n",
      "Config 6/11: bge-large-en_vanilla_k3_GPT-5-mini_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "====================================================================================================\n",
      "Config 7/11: bge-large-en_vanilla_k3_Claude-Sonnet-4.5_baseline\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "====================================================================================================\n",
      "Config 8/11: bge-large-en_vanilla_k3_Llama-4-Maverick_citation_focused\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.24s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.82s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.36s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 3.30s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.59s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 57.53%\n",
      "     Avg Response Time: 2.66s\n",
      "\n",
      "====================================================================================================\n",
      "Config 9/11: bge-large-en_vanilla_k3_Llama-4-Maverick_few_shot\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 0.87s\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.51s\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.96s\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 2.77s\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âœ… Generated in 1.08s\n",
      "\n",
      "  ğŸ“Š Config Summary:\n",
      "     Avg LLM Judge Score: 61.51%\n",
      "     Avg Response Time: 1.64s\n",
      "\n",
      "====================================================================================================\n",
      "Config 10/11: bge-large-en_reranked_k3_GPT-5-mini_citation_focused\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "====================================================================================================\n",
      "Config 11/11: bge-large-en_mmr_balanced_Claude-Sonnet-4.5_citation_focused\n",
      "====================================================================================================\n",
      "\n",
      "  Example1: Daha az quyu ilÉ™ daha Ã§ox hasilat É™ldÉ™ etmÉ™k Ã¼Ã§Ã¼n hansÄ± É™sas...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example2: QÉ™rbi AbÅŸeron yataÄŸÄ±nda suvurma tÉ™dbirlÉ™ri hansÄ± tarixdÉ™ vÉ™ ...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example3: PirallahÄ± strukturunda 1253 nÃ¶mrÉ™li quyudan gÃ¶tÃ¼rÃ¼lmÃ¼ÅŸ nÃ¼mun...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example4: BakÄ± arxipelaqÄ± (BA) vÉ™ AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) Ã¼Ã§Ã¼n geote...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Example5: Bu zonada hansÄ± proseslÉ™r baÅŸ verir?...\n",
      "    Retrieved 3 docs\n",
      "    âŒ ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "====================================================================================================\n",
      "âœ… Comprehensive benchmark complete!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for config_idx, (embed_key, retrieval_key, llm_key, prompt_key) in enumerate(CONFIGS_TO_TEST, 1):\n",
    "    config_name = f\"{embed_key}_{retrieval_key}_{llm_key}_{prompt_key}\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Config {config_idx}/{len(CONFIGS_TO_TEST)}: {config_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Get components\n",
    "    embed_model = embedding_cache[embed_key]\n",
    "    retrieval_func = RETRIEVAL_STRATEGIES[retrieval_key]['func']\n",
    "    retrieval_params = RETRIEVAL_STRATEGIES[retrieval_key]['params']\n",
    "    \n",
    "    config_results = []\n",
    "    \n",
    "    for example_key, messages in questions.items():\n",
    "        user_msg = [m for m in messages if m['role'] == 'user'][-1]\n",
    "        query = user_msg['content']\n",
    "        \n",
    "        print(f\"\\n  {example_key}: {query[:60]}...\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        documents = retrieval_func(query, embed_model, **retrieval_params)\n",
    "        print(f\"    Retrieved {len(documents)} docs\")\n",
    "        \n",
    "        # Generate answer\n",
    "        answer, response_time = generate_answer(llm_key, query, documents, prompt_key)\n",
    "        \n",
    "        if answer.startswith('ERROR'):\n",
    "            print(f\"    âŒ {answer}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    âœ… Generated in {response_time:.2f}s\")\n",
    "        \n",
    "        # Evaluate\n",
    "        expected = expected_answers.get(example_key, {}).get('Answer', '')\n",
    "        \n",
    "        accuracy_metrics = calculate_answer_quality(expected, answer) if expected else {'Accuracy_Score': 0}\n",
    "        citation_metrics = evaluate_citation_quality(answer, documents)\n",
    "        retrieval_metrics = evaluate_retrieval_quality(query, documents, expected)\n",
    "        completeness_metrics = evaluate_completeness(answer)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        llm_judge_score = calculate_llm_judge_score(\n",
    "            accuracy_metrics['Accuracy_Score'],\n",
    "            citation_metrics['Citation_Score'],\n",
    "            completeness_metrics['Completeness_Score']\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'Config': config_name,\n",
    "            'Embedding_Model': embed_key,\n",
    "            'Retrieval_Strategy': retrieval_key,\n",
    "            'LLM_Model': llm_key,\n",
    "            'Prompt_Strategy': prompt_key,\n",
    "            'Question': example_key,\n",
    "            'Query': query[:80],\n",
    "            'Num_Docs_Retrieved': len(documents),\n",
    "            'Response_Time': round(response_time, 2),\n",
    "            'LLM_Judge_Score': llm_judge_score,\n",
    "            **accuracy_metrics,\n",
    "            **citation_metrics,\n",
    "            **retrieval_metrics,\n",
    "            **completeness_metrics,\n",
    "            'Answer_Preview': answer[:150]\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        config_results.append(result)\n",
    "    \n",
    "    # Show config summary\n",
    "    if config_results:\n",
    "        avg_score = sum(r['LLM_Judge_Score'] for r in config_results) / len(config_results)\n",
    "        avg_time = sum(r['Response_Time'] for r in config_results) / len(config_results)\n",
    "        print(f\"\\n  ğŸ“Š Config Summary:\")\n",
    "        print(f\"     Avg LLM Judge Score: {avg_score:.2f}%\")\n",
    "        print(f\"     Avg Response Time: {avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"âœ… Comprehensive benchmark complete!\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "ğŸ“Š CONFIGURATION RANKINGS (By LLM Judge Score)\n",
      "========================================================================================================================\n",
      "                                                                  Embedding_Model Retrieval_Strategy         LLM_Model   Prompt_Strategy  LLM_Judge_Score  Accuracy_Score  Citation_Score  Response_Time\n",
      "Config                                                                                                                                                                                                  \n",
      "bge-large-en_vanilla_k3_Llama-4-Maverick_few_shot                    bge-large-en         vanilla_k3  Llama-4-Maverick          few_shot            61.51           11.35           78.67           1.64\n",
      "bge-large-en_vanilla_k3_Llama-4-Maverick_citation_focused            bge-large-en         vanilla_k3  Llama-4-Maverick  citation_focused            57.53            0.00           78.67           2.66\n",
      "bge-large-en_mmr_balanced_Llama-4-Maverick_baseline                  bge-large-en       mmr_balanced  Llama-4-Maverick          baseline            45.40            0.00           44.00           2.16\n",
      "bge-large-en_vanilla_k5_Llama-4-Maverick_baseline                    bge-large-en         vanilla_k5  Llama-4-Maverick          baseline            45.40            0.00           44.00           2.89\n",
      "bge-large-en_reranked_k3_Llama-4-Maverick_baseline                   bge-large-en        reranked_k3  Llama-4-Maverick          baseline            44.47            0.00           41.33           3.08\n",
      "bge-large-en_vanilla_k3_Llama-4-Maverick_baseline                    bge-large-en         vanilla_k3  Llama-4-Maverick          baseline            43.53            0.00           38.67           2.50\n",
      "multilingual-e5-large_vanilla_k3_Llama-4-Maverick_baseline  multilingual-e5-large         vanilla_k3  Llama-4-Maverick          baseline            39.73            0.00           28.00           3.31\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate by configuration\n",
    "config_summary = df.groupby('Config').agg({\n",
    "    'LLM_Judge_Score': 'mean',\n",
    "    'Accuracy_Score': 'mean',\n",
    "    'Citation_Score': 'mean',\n",
    "    'Retrieval_Relevance': 'mean',\n",
    "    'Completeness_Score': 'mean',\n",
    "    'Response_Time': 'mean',\n",
    "    'Embedding_Model': 'first',\n",
    "    'Retrieval_Strategy': 'first',\n",
    "    'LLM_Model': 'first',\n",
    "    'Prompt_Strategy': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# Sort by LLM Judge Score\n",
    "config_summary = config_summary.sort_values('LLM_Judge_Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ğŸ“Š CONFIGURATION RANKINGS (By LLM Judge Score)\")\n",
    "print(\"=\"*120)\n",
    "display_cols = ['Embedding_Model', 'Retrieval_Strategy', 'LLM_Model', 'Prompt_Strategy', \n",
    "                'LLM_Judge_Score', 'Accuracy_Score', 'Citation_Score', 'Response_Time']\n",
    "print(config_summary[display_cols].to_string())\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ” COMPONENT IMPACT ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“š EMBEDDING MODELS:\n",
      "   bge-large-en: 49.64%\n",
      "   multilingual-e5-large: 39.73%\n",
      "\n",
      "ğŸ” RETRIEVAL STRATEGIES:\n",
      "   vanilla_k3: 50.58% (Current setup)\n",
      "   mmr_balanced: 45.40% (Balance diversity)\n",
      "   vanilla_k5: 45.40% (More context)\n",
      "   reranked_k3: 44.47% (Two-stage rerank)\n",
      "\n",
      "ğŸ¤– LLM MODELS:\n",
      "   Llama-4-Maverick: 48.22%\n",
      "\n",
      "ğŸ’¬ PROMPTING STRATEGIES:\n",
      "   few_shot: 61.51%\n",
      "   citation_focused: 57.53%\n",
      "   baseline: 43.71%\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze impact of each component\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ” COMPONENT IMPACT ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Embedding Models\n",
    "print(\"\\nğŸ“š EMBEDDING MODELS:\")\n",
    "embed_impact = df.groupby('Embedding_Model')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for model, score in embed_impact.items():\n",
    "    print(f\"   {model}: {score:.2f}%\")\n",
    "\n",
    "# 2. Retrieval Strategies\n",
    "print(\"\\nğŸ” RETRIEVAL STRATEGIES:\")\n",
    "retrieval_impact = df.groupby('Retrieval_Strategy')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for strategy, score in retrieval_impact.items():\n",
    "    notes = RETRIEVAL_STRATEGIES[strategy]['notes']\n",
    "    print(f\"   {strategy}: {score:.2f}% ({notes})\")\n",
    "\n",
    "# 3. LLM Models\n",
    "print(\"\\nğŸ¤– LLM MODELS:\")\n",
    "llm_impact = df.groupby('LLM_Model')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for model, score in llm_impact.items():\n",
    "    print(f\"   {model}: {score:.2f}%\")\n",
    "\n",
    "# 4. Prompting Strategies\n",
    "print(\"\\nğŸ’¬ PROMPTING STRATEGIES:\")\n",
    "prompt_impact = df.groupby('Prompt_Strategy')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for strategy, score in prompt_impact.items():\n",
    "    print(f\"   {strategy}: {score:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# Create output directory\noutput_dir = Path('output/rag_optimization_benchmark')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. Top Configurations\nax1 = axes[0, 0]\ntop_configs = config_summary.head(10)\nconfig_labels = [c.split('_')[-2] + '+' + c.split('_')[-1] for c in top_configs.index]\nax1.barh(config_labels, top_configs['LLM_Judge_Score'], color=sns.color_palette('viridis', len(top_configs)))\nax1.set_xlabel('LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax1.set_title('Top 10 Configurations', fontsize=13, fontweight='bold')\nax1.set_xlim(0, 100)\nfor i, score in enumerate(top_configs['LLM_Judge_Score']):\n    ax1.text(score + 1, i, f'{score:.1f}', va='center', fontsize=10)\n\n# 2. Embedding Model Impact\nax2 = axes[0, 1]\nax2.bar(embed_impact.index, embed_impact.values, color='skyblue', alpha=0.8)\nax2.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax2.set_title('Embedding Model Impact', fontsize=13, fontweight='bold')\nax2.set_ylim(0, 100)\nax2.tick_params(axis='x', rotation=45)\nfor i, (model, score) in enumerate(embed_impact.items()):\n    ax2.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 3. Retrieval Strategy Impact\nax3 = axes[0, 2]\nax3.bar(retrieval_impact.index, retrieval_impact.values, color='coral', alpha=0.8)\nax3.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax3.set_title('Retrieval Strategy Impact', fontsize=13, fontweight='bold')\nax3.set_ylim(0, 100)\nax3.tick_params(axis='x', rotation=45)\nfor i, (strategy, score) in enumerate(retrieval_impact.items()):\n    ax3.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=9)\n\n# 4. LLM Model Impact\nax4 = axes[1, 0]\nax4.bar(llm_impact.index, llm_impact.values, color='mediumseagreen', alpha=0.8)\nax4.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax4.set_title('LLM Model Impact', fontsize=13, fontweight='bold')\nax4.set_ylim(0, 100)\nax4.tick_params(axis='x', rotation=45)\nfor i, (model, score) in enumerate(llm_impact.items()):\n    ax4.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 5. Prompting Strategy Impact\nax5 = axes[1, 1]\nax5.bar(prompt_impact.index, prompt_impact.values, color='mediumpurple', alpha=0.8)\nax5.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax5.set_title('Prompting Strategy Impact', fontsize=13, fontweight='bold')\nax5.set_ylim(0, 100)\nax5.tick_params(axis='x', rotation=45)\nfor i, (strategy, score) in enumerate(prompt_impact.items()):\n    ax5.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 6. Score Components (best config)\nax6 = axes[1, 2]\nbest_config = config_summary.iloc[0]\ncomponents = ['Accuracy', 'Citation', 'Completeness']\nscores = [best_config['Accuracy_Score'], best_config['Citation_Score'], best_config['Completeness_Score']]\ncolors_comp = ['#FF6B6B', '#4ECDC4', '#45B7D1']\nbars = ax6.bar(components, scores, color=colors_comp, alpha=0.8)\nax6.set_ylabel('Score (%)', fontsize=11, fontweight='bold')\nax6.set_title(f'Best Config Components\\n{best_config.name.split(\"_\")[2]}', fontsize=13, fontweight='bold')\nax6.set_ylim(0, 100)\nfor i, score in enumerate(scores):\n    ax6.text(i, score + 2, f'{score:.1f}%', ha='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nâœ… Visualization saved to '{output_dir}/results.png'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ† OPTIMAL RAG CONFIGURATION\n",
      "====================================================================================================\n",
      "\n",
      "âœ… Best Configuration: bge-large-en_vanilla_k3_Llama-4-Maverick_few_shot\n",
      "\n",
      "ğŸ“Š Performance:\n",
      "   LLM Judge Score: 61.51%\n",
      "   Accuracy: 11.35%\n",
      "   Citation Quality: 78.67%\n",
      "   Completeness: 100.00%\n",
      "   Avg Response Time: 1.64s\n",
      "\n",
      "âš™ï¸ Components:\n",
      "   Embedding Model: bge-large-en\n",
      "      â†’ BAAI/bge-large-en-v1.5\n",
      "   Retrieval Strategy: vanilla_k3\n",
      "      â†’ Current setup\n",
      "   LLM Model: Llama-4-Maverick\n",
      "   Prompting Strategy: few_shot\n",
      "\n",
      "ğŸ’¡ Key Findings:\n",
      "   1. Best Embedding: bge-large-en (49.64%)\n",
      "   2. Best Retrieval: vanilla_k3 (50.58%)\n",
      "   3. Best LLM: Llama-4-Maverick (48.22%)\n",
      "   4. Best Prompt: few_shot (61.51%)\n",
      "\n",
      "ğŸ¯ Hackathon Impact:\n",
      "   LLM Quality = 30% of total score\n",
      "   Your score: 61.51% Ã— 30% = 18.45 points\n",
      "\n",
      "ğŸ“ˆ Improvement vs Baseline:\n",
      "   +24.51% quality improvement\n",
      "   = +7.35 hackathon points\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“ IMPLEMENTATION CHECKLIST\n",
      "====================================================================================================\n",
      "\n",
      "1. Use embedding model: BAAI/bge-large-en-v1.5\n",
      "2. Implement retrieval: vanilla_k3\n",
      "3. Use LLM model: Llama-4-Maverick\n",
      "4. Apply prompt: few_shot\n",
      "\n",
      "5. Expected performance:\n",
      "   - LLM Judge Score: 61.51%\n",
      "   - Response time: ~1.6s\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_config = config_summary.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ† OPTIMAL RAG CONFIGURATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nâœ… Best Configuration: {best_config.name}\")\n",
    "print(f\"\\nğŸ“Š Performance:\")\n",
    "print(f\"   LLM Judge Score: {best_config['LLM_Judge_Score']:.2f}%\")\n",
    "print(f\"   Accuracy: {best_config['Accuracy_Score']:.2f}%\")\n",
    "print(f\"   Citation Quality: {best_config['Citation_Score']:.2f}%\")\n",
    "print(f\"   Completeness: {best_config['Completeness_Score']:.2f}%\")\n",
    "print(f\"   Avg Response Time: {best_config['Response_Time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Components:\")\n",
    "print(f\"   Embedding Model: {best_config['Embedding_Model']}\")\n",
    "print(f\"      â†’ {EMBEDDING_MODELS[best_config['Embedding_Model']]['name']}\")\n",
    "print(f\"   Retrieval Strategy: {best_config['Retrieval_Strategy']}\")\n",
    "print(f\"      â†’ {RETRIEVAL_STRATEGIES[best_config['Retrieval_Strategy']]['notes']}\")\n",
    "print(f\"   LLM Model: {best_config['LLM_Model']}\")\n",
    "print(f\"   Prompting Strategy: {best_config['Prompt_Strategy']}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Key Findings:\")\n",
    "print(f\"   1. Best Embedding: {embed_impact.index[0]} ({embed_impact.values[0]:.2f}%)\")\n",
    "print(f\"   2. Best Retrieval: {retrieval_impact.index[0]} ({retrieval_impact.values[0]:.2f}%)\")\n",
    "print(f\"   3. Best LLM: {llm_impact.index[0]} ({llm_impact.values[0]:.2f}%)\")\n",
    "print(f\"   4. Best Prompt: {prompt_impact.index[0]} ({prompt_impact.values[0]:.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Hackathon Impact:\")\n",
    "print(f\"   LLM Quality = 30% of total score\")\n",
    "print(f\"   Your score: {best_config['LLM_Judge_Score']:.2f}% Ã— 30% = {best_config['LLM_Judge_Score'] * 0.3:.2f} points\")\n",
    "\n",
    "baseline = df[df['Config'].str.contains('baseline')].iloc[0] if len(df[df['Config'].str.contains('baseline')]) > 0 else None\n",
    "if baseline is not None:\n",
    "    improvement = best_config['LLM_Judge_Score'] - baseline['LLM_Judge_Score']\n",
    "    print(f\"\\nğŸ“ˆ Improvement vs Baseline:\")\n",
    "    print(f\"   +{improvement:.2f}% quality improvement\")\n",
    "    print(f\"   = +{improvement * 0.3:.2f} hackathon points\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ“ IMPLEMENTATION CHECKLIST\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n1. Use embedding model: {EMBEDDING_MODELS[best_config['Embedding_Model']]['name']}\")\n",
    "print(f\"2. Implement retrieval: {best_config['Retrieval_Strategy']}\")\n",
    "print(f\"3. Use LLM model: {best_config['LLM_Model']}\")\n",
    "print(f\"4. Apply prompt: {best_config['Prompt_Strategy']}\")\n",
    "print(f\"\\n5. Expected performance:\")\n",
    "print(f\"   - LLM Judge Score: {best_config['LLM_Judge_Score']:.2f}%\")\n",
    "print(f\"   - Response time: ~{best_config['Response_Time']:.1f}s\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nfrom pathlib import Path\n\noutput_dir = Path('output/rag_optimization_benchmark')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\ndf.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\nconfig_summary.to_csv(output_dir / 'summary.csv', encoding='utf-8')\n\n# Save component impacts\nimpacts = pd.DataFrame({\n    'Embedding_Impact': embed_impact,\n    'Retrieval_Impact': retrieval_impact.reindex(embed_impact.index, fill_value=0),\n    'LLM_Impact': llm_impact.reindex(embed_impact.index, fill_value=0),\n    'Prompt_Impact': prompt_impact.reindex(embed_impact.index, fill_value=0)\n}).fillna(0)\nimpacts.to_csv(output_dir / 'component_impacts.csv', encoding='utf-8')\n\nprint(\"\\nâœ… Results exported to output/rag_optimization_benchmark/:\")\nprint(\"   - detailed_results.csv (all tests)\")\nprint(\"   - summary.csv (config rankings)\")\nprint(\"   - component_impacts.csv (component analysis)\")\nprint(\"   - results.png (visualizations)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}