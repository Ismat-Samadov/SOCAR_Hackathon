{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Benchmarking for SOCAR Hackathon RAG Chatbot\n",
    "\n",
    "This notebook tests different LLM models for the `/llm` endpoint to find the best performer.\n",
    "\n",
    "## Evaluation Criteria (LLM Judge Metrics):\n",
    "- **Accuracy**: Is the answer correct?\n",
    "- **Relevance**: Are retrieved citations relevant?\n",
    "- **Completeness**: Does it fully answer the question?\n",
    "- **Citation Quality**: Proper sources with page numbers?\n",
    "- **Response Time**: Speed of generation\n",
    "\n",
    "## Available LLM Models:\n",
    "1. **Llama-4-Maverick-17B-128E-Instruct-FP8** (Current choice, open-source)\n",
    "2. **DeepSeek-R1** (Open-source reasoning model)\n",
    "3. **GPT-4.1** (Strong general performance)\n",
    "4. **GPT-5, GPT-5-mini**\n",
    "5. **Claude Sonnet 4.5** (Best quality)\n",
    "6. **Claude Opus 4.1**\n",
    "7. **Phi-4-multimodal-instruct**\n",
    "8. **gpt-oss-120b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai pinecone-client sentence-transformers python-dotenv pandas matplotlib seaborn jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/SOCAR_Hackathon/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import AzureOpenAI\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"\u2705 Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Project root: /Users/ismatsamadov/SOCAR_Hackathon\n",
      "\u2705 Docs directory: /Users/ismatsamadov/SOCAR_Hackathon/docs\n",
      "\u2705 Output directory: /Users/ismatsamadov/SOCAR_Hackathon/output\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect project root (works from any directory)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('data').exists() and Path('docs').exists():\n",
    "    # Already in project root\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif Path('../data').exists() and Path('../docs').exists():\n",
    "    # In notebooks/ subdirectory\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    # Fallback: try to find project root\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'data').exists() and (current / 'docs').exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define all paths relative to project root\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "DOCS_DIR = PROJECT_ROOT / 'docs'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "\n",
    "print(f\"\u2705 Project root: {PROJECT_ROOT}\")\n",
    "print(f\"\u2705 Docs directory: {DOCS_DIR}\")\n",
    "print(f\"\u2705 Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 test cases\n",
      "\n",
      "Test Questions:\n",
      "1. Example1: Daha az quyu il\u0259 daha \u00e7ox hasilat \u0259ld\u0259 etm\u0259k \u00fc\u00e7\u00fcn hans\u0131 \u0259sas amill\u0259rin inteqrasiyas\u0131 t\u0259l\u0259b olunur?...\n",
      "2. Example2: Q\u0259rbi Ab\u015feron yata\u011f\u0131nda suvurma t\u0259dbirl\u0259ri hans\u0131 tarixd\u0259 v\u0259 hans\u0131 layda t\u0259tbiq edilmi\u015fdir v\u0259 bunun m...\n",
      "3. Example3: Pirallah\u0131 strukturunda 1253 n\u00f6mr\u0259li quyudan g\u00f6t\u00fcr\u00fclm\u00fc\u015f n\u00fcmun\u0259l\u0259rd\u0259 SiO2 v\u0259 CaO oksidl\u0259ri aras\u0131nda ha...\n",
      "4. Example4: Bak\u0131 arxipelaq\u0131 (BA) v\u0259 A\u015fa\u011f\u0131 K\u00fcr \u00e7\u00f6k\u0259kliyi (AK\u00c7) \u00fc\u00e7\u00fcn geotemperatur x\u0259rit\u0259l\u0259rin\u0259 \u0259sas\u0259n neft v\u0259 qaz...\n",
      "5. Example5: Bu zonada hans\u0131 prosesl\u0259r ba\u015f verir?...\n"
     ]
    }
   ],
   "source": [
    "# Load sample questions - using dynamic paths\n",
    "with open(DOCS_DIR / 'sample_questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Load expected answers - using dynamic paths\n",
    "with open(DOCS_DIR / 'sample_answers.json', 'r', encoding='utf-8') as f:\n",
    "    expected_answers = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(questions)} test cases\")\n",
    "print(\"\\nTest Questions:\")\n",
    "for i, (key, msgs) in enumerate(questions.items(), 1):\n",
    "    user_msg = [m for m in msgs if m['role'] == 'user'][-1]\n",
    "    print(f\"{i}. {key}: {user_msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Vector Database and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "index = pc.Index(os.getenv('PINECONE_INDEX_NAME', 'hackathon'))\n",
    "\n",
    "# Initialize embedding model (same as used for ingestion)\n",
    "embed_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "\n",
    "print(f\"\u2705 Vector DB connected: {index.describe_index_stats()}\")\n",
    "print(f\"\u2705 Embedding model loaded: {embed_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from vector database.\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    # Search vector DB\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Extract documents\n",
    "    documents = []\n",
    "    for match in results['matches']:\n",
    "        documents.append({\n",
    "            'pdf_name': match['metadata'].get('pdf_name', 'unknown.pdf'),\n",
    "            'page_number': match['metadata'].get('page_number', 0),\n",
    "            'content': match['metadata'].get('text', ''),\n",
    "            'score': match.get('score', 0.0)\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"Pal\u00e7\u0131q vulkanlar\u0131n\u0131n t\u0259sir radiusu n\u0259 q\u0259d\u0259rdir?\"\n",
    "test_docs = retrieve_documents(test_query)\n",
    "print(f\"\\n\u2705 Retrieved {len(test_docs)} documents for test query\")\n",
    "print(f\"Top result: {test_docs[0]['pdf_name']}, page {test_docs[0]['page_number']} (score: {test_docs[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM Client Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    ")\n",
    "\n",
    "LLM_MODELS = {\n",
    "    'Llama-4-Maverick': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
    "    'DeepSeek-R1': 'DeepSeek-R1',\n",
    "    'GPT-4.1': 'gpt-4.1',\n",
    "    'GPT-5-mini': 'gpt-5-mini',\n",
    "    'Claude-Sonnet-4.5': 'claude-sonnet-4-5',\n",
    "}\n",
    "\n",
    "def generate_answer(model_name: str, query: str, documents: List[Dict], \n",
    "                   temperature: float = 0.2, max_tokens: int = 1000) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generate answer using specified LLM model.\n",
    "    Returns: (answer, response_time)\n",
    "    \"\"\"\n",
    "    # Build context from retrieved documents\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        context_parts.append(\n",
    "            f\"Document {i} (Source: {doc['pdf_name']}, Page {doc['page_number']}):\\n{doc['content']}\"\n",
    "        )\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Siz SOCAR-\u0131n tarixi neft v\u0259 qaz s\u0259n\u0259dl\u0259ri \u00fczr\u0259 m\u00fct\u0259x\u0259ssis k\u00f6m\u0259k\u00e7isisiniz.\n",
    "\n",
    "Kontekst (\u0259laq\u0259li s\u0259n\u0259dl\u0259r):\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "\u018ftrafl\u0131 cavab verin v\u0259 m\u00fctl\u0259q s\u0259n\u0259d m\u0259nb\u0259l\u0259rin\u0259 istinad edin (PDF ad\u0131 v\u0259 s\u0259hif\u0259 n\u00f6mr\u0259si il\u0259).\n",
    "Cavab\u0131n\u0131z d\u0259qiq, faktlara \u0259saslanan v\u0259 kontekst m\u0259lumatlar\u0131ndan istifad\u0259 ed\u0259n olmal\u0131d\u0131r.\"\"\"\n",
    "    \n",
    "    # Get model deployment\n",
    "    deployment = MODELS[model_name]['deployment']\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # GPT-5 models use max_completion_tokens, others use max_tokens\n",
    "        if deployment.startswith('gpt-5'):\n",
    "            response = azure_client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_completion_tokens=max_tokens\n",
    "            )\n",
    "        else:\n",
    "            response = azure_client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        return answer, response_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\", 0.0\n",
    "\n",
    "print(f\"\\n\u2705 Configured {len(LLM_MODELS)} LLM models for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    import re\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_answer_similarity(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate similarity between generated and expected answer.\n",
    "    Lower is better for error rates.\n",
    "    \"\"\"\n",
    "    ref_norm = normalize_text(reference)\n",
    "    hyp_norm = normalize_text(hypothesis)\n",
    "    \n",
    "    # Character Error Rate\n",
    "    cer_score = cer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Word Error Rate  \n",
    "    wer_score = wer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Similarity scores (higher is better)\n",
    "    similarity = max(0, 100 - wer_score)\n",
    "    \n",
    "    return {\n",
    "        'CER': round(cer_score, 2),\n",
    "        'WER': round(wer_score, 2),\n",
    "        'Similarity': round(similarity, 2)\n",
    "    }\n",
    "\n",
    "def check_citations(answer: str, documents: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Check if answer includes proper citations.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Check for PDF names\n",
    "    pdf_names = [doc['pdf_name'] for doc in documents]\n",
    "    cited_pdfs = sum(1 for pdf in pdf_names if pdf.replace('.pdf', '') in answer)\n",
    "    \n",
    "    # Check for page numbers\n",
    "    page_numbers = [str(doc['page_number']) for doc in documents]\n",
    "    cited_pages = sum(1 for page in page_numbers if page in answer)\n",
    "    \n",
    "    # Check for source keywords\n",
    "    source_keywords = ['m\u0259nb\u0259', 's\u0259n\u0259d', 's\u0259hif\u0259', 'pdf', 'document', 'page', 'source']\n",
    "    has_source_ref = any(kw in answer.lower() for kw in source_keywords)\n",
    "    \n",
    "    citation_score = (\n",
    "        (cited_pdfs / len(pdf_names) * 40) +  # 40% for PDF citation\n",
    "        (cited_pages / len(page_numbers) * 40) +  # 40% for page citation\n",
    "        (20 if has_source_ref else 0)  # 20% for having source keywords\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Citation_Score': round(citation_score, 2),\n",
    "        'Cited_PDFs': cited_pdfs,\n",
    "        'Cited_Pages': cited_pages,\n",
    "        'Has_Source_Reference': has_source_ref\n",
    "    }\n",
    "\n",
    "def evaluate_completeness(answer: str, min_length: int = 100) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Evaluate answer completeness.\n",
    "    \"\"\"\n",
    "    word_count = len(answer.split())\n",
    "    char_count = len(answer)\n",
    "    \n",
    "    # Penalize very short or very long answers\n",
    "    if char_count < min_length:\n",
    "        completeness_score = (char_count / min_length) * 100\n",
    "    elif char_count > 2000:\n",
    "        completeness_score = 100 - ((char_count - 2000) / 2000 * 20)  # Penalty for verbosity\n",
    "    else:\n",
    "        completeness_score = 100\n",
    "    \n",
    "    return {\n",
    "        'Completeness_Score': round(max(0, completeness_score), 2),\n",
    "        'Word_Count': word_count,\n",
    "        'Char_Count': char_count\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Benchmark on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models to test (you can comment out models to skip)\n",
    "MODELS_TO_TEST = [\n",
    "    'Llama-4-Maverick-17B',\n",
    "    'DeepSeek-R1',\n",
    "    'GPT-4.1',\n",
    "    'GPT-5-mini',\n",
    "    'Claude-Sonnet-4.5',\n",
    "    # 'Claude-Opus-4.1',  # Uncomment to test\n",
    "    # 'Phi-4-multimodal',  # Uncomment to test\n",
    "    # 'GPT-OSS-120B',  # Uncomment to test\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {len(questions)} questions...\\n\")\n",
    "print(\"This may take several minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for example_key, messages in questions.items():\n",
    "        # Get the last user message (the actual question)\n",
    "        user_msg = [m for m in messages if m['role'] == 'user'][-1]\n",
    "        query = user_msg['content']\n",
    "        \n",
    "        print(f\"\\n  Question {example_key}: {query[:80]}...\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        documents = retrieve_documents(query, top_k=3)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer, response_time = generate_answer(model_name, query, documents)\n",
    "        \n",
    "        if answer.startswith('ERROR'):\n",
    "            print(f\"  \u274c Failed: {answer}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  \u2705 Response time: {response_time:.2f}s\")\n",
    "        \n",
    "        # Get expected answer\n",
    "        expected = expected_answers.get(example_key, {}).get('Answer', '')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        similarity_metrics = calculate_answer_similarity(expected, answer) if expected else {'CER': 0, 'WER': 0, 'Similarity': 0}\n",
    "        citation_metrics = check_citations(answer, documents)\n",
    "        completeness_metrics = evaluate_completeness(answer)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Question': example_key,\n",
    "            'Query': query[:100],\n",
    "            'Answer': answer[:200] + '...',\n",
    "            'Response_Time': round(response_time, 2),\n",
    "            **similarity_metrics,\n",
    "            **citation_metrics,\n",
    "            **completeness_metrics,\n",
    "            'Open_Source': MODELS[model_name]['open_source'],\n",
    "            'Architecture_Score': MODELS[model_name]['architecture_score']\n",
    "        }\n",
    "        \n",
    "        model_results.append(result)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Show summary for this model\n",
    "    if model_results:\n",
    "        avg_response_time = sum(r['Response_Time'] for r in model_results) / len(model_results)\n",
    "        avg_similarity = sum(r['Similarity'] for r in model_results) / len(model_results)\n",
    "        avg_citation = sum(r['Citation_Score'] for r in model_results) / len(model_results)\n",
    "        avg_completeness = sum(r['Completeness_Score'] for r in model_results) / len(model_results)\n",
    "        \n",
    "        print(f\"\\n  \ud83d\udcca {model_name} Summary:\")\n",
    "        print(f\"     Avg Response Time: {avg_response_time:.2f}s\")\n",
    "        print(f\"     Avg Similarity: {avg_similarity:.1f}%\")\n",
    "        print(f\"     Avg Citation Score: {avg_citation:.1f}%\")\n",
    "        print(f\"     Avg Completeness: {avg_completeness:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\u2705 Benchmarking complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregate Results and Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate aggregate scores per model\n",
    "model_summary = df.groupby('Model').agg({\n",
    "    'Response_Time': 'mean',\n",
    "    'Similarity': 'mean',\n",
    "    'Citation_Score': 'mean',\n",
    "    'Completeness_Score': 'mean',\n",
    "    'CER': 'mean',\n",
    "    'WER': 'mean',\n",
    "    'Open_Source': 'first',\n",
    "    'Architecture_Score': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# Calculate overall quality score (weighted average)\n",
    "model_summary['Quality_Score'] = (\n",
    "    model_summary['Similarity'] * 0.35 +  # 35% answer accuracy\n",
    "    model_summary['Citation_Score'] * 0.35 +  # 35% citation quality\n",
    "    model_summary['Completeness_Score'] * 0.30  # 30% completeness\n",
    ").round(2)\n",
    "\n",
    "# Sort by Quality Score\n",
    "model_summary = model_summary.sort_values('Quality_Score', ascending=False)\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83d\udcca LLM BENCHMARKING RESULTS - MODEL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(model_summary.to_string())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create comprehensive visualization\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory - using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'llm_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "models = df['Model'].tolist()\n",
    "colors = sns.color_palette('viridis', len(models))\n",
    "\n",
    "# 1. CSR - Character Success Rate (MAIN METRIC)\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(models, df['CSR'], color=colors)\n",
    "ax1.set_xlabel('CSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Character Success Rate (CSR)\\n\ud83c\udfc6 HACKATHON PRIMARY METRIC', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "for i, (model, csr) in enumerate(zip(models, df['CSR'])):\n",
    "    ax1.text(csr + 1, i, f'{csr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "ax1.axvline(x=90, color='green', linestyle='--', alpha=0.3, label='Excellent (>90%)')\n",
    "ax1.axvline(x=80, color='orange', linestyle='--', alpha=0.3, label='Good (>80%)')\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# 2. WSR - Word Success Rate\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.barh(models, df['WSR'], color=colors)\n",
    "ax2.set_xlabel('WSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Word Success Rate (WSR)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, 100)\n",
    "for i, (model, wsr) in enumerate(zip(models, df['WSR'])):\n",
    "    ax2.text(wsr + 1, i, f'{wsr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Response Time\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.barh(models, df['Response_Time'], color=colors)\n",
    "ax3.set_xlabel('Total Time (seconds) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Processing Speed', fontsize=14, fontweight='bold')\n",
    "for i, (model, time_val) in enumerate(zip(models, df['Response_Time'])):\n",
    "    ax3.text(time_val + 0.5, i, f'{time_val:.1f}s', va='center', fontsize=11)\n",
    "\n",
    "# 4. Error Rates Comparison\n",
    "ax4 = axes[1, 1]\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "ax4.bar([i - width/2 for i in x], df['CER'], width, label='CER', color='coral', alpha=0.8)\n",
    "ax4.bar([i + width/2 for i in x], df['WER'], width, label='WER', color='skyblue', alpha=0.8)\n",
    "ax4.set_ylabel('Error Rate (%) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Error Rates', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u2705 Visualization saved to '{output_dir}/results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rankings table\n",
    "rankings = model_summary[[\n",
    "    'Quality_Score', 'Similarity', 'Citation_Score', 'Completeness_Score', \n",
    "    'Response_Time', 'Open_Source', 'Architecture_Score'\n",
    "]].copy()\n",
    "\n",
    "rankings.insert(0, 'Rank', range(1, len(rankings) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83c\udfc6 FINAL RANKINGS\")\n",
    "print(\"=\"*100)\n",
    "print(rankings.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Winner analysis\n",
    "best_overall = rankings.index[0]\n",
    "best_open_source = rankings[rankings['Open_Source'] == True].index[0] if any(rankings['Open_Source']) else None\n",
    "fastest = model_summary['Response_Time'].idxmin()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83d\udca1 RECOMMENDATIONS FOR HACKATHON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n\ud83e\udd47 Best Overall Quality: {best_overall}\")\n",
    "print(f\"   Quality Score: {model_summary.loc[best_overall, 'Quality_Score']:.1f}%\")\n",
    "print(f\"   Similarity: {model_summary.loc[best_overall, 'Similarity']:.1f}%\")\n",
    "print(f\"   Citation Score: {model_summary.loc[best_overall, 'Citation_Score']:.1f}%\")\n",
    "print(f\"   Response Time: {model_summary.loc[best_overall, 'Response_Time']:.2f}s\")\n",
    "print(f\"   Open Source: {model_summary.loc[best_overall, 'Open_Source']}\")\n",
    "print(f\"   Architecture Score: {model_summary.loc[best_overall, 'Architecture_Score']}\")\n",
    "\n",
    "if best_open_source:\n",
    "    print(f\"\\n\ud83d\udd13 Best Open-Source Model: {best_open_source}\")\n",
    "    print(f\"   Quality Score: {model_summary.loc[best_open_source, 'Quality_Score']:.1f}%\")\n",
    "    print(f\"   Architecture Score: {model_summary.loc[best_open_source, 'Architecture_Score']} (Better for hackathon!)\")\n",
    "    print(f\"   Response Time: {model_summary.loc[best_open_source, 'Response_Time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n\u26a1 Fastest Model: {fastest}\")\n",
    "print(f\"   Response Time: {model_summary.loc[fastest, 'Response_Time']:.2f}s\")\n",
    "print(f\"   Quality Score: {model_summary.loc[fastest, 'Quality_Score']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"\ud83d\udcdd FINAL RECOMMENDATION\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nScoring Breakdown:\")\n",
    "print(\"  - LLM Quality: 30% of total hackathon score\")\n",
    "print(\"  - Architecture: 20% of total hackathon score (open-source preferred!)\")\n",
    "print(\"\\nBest Choice:\")\n",
    "if best_open_source and model_summary.loc[best_open_source, 'Quality_Score'] >= model_summary.loc[best_overall, 'Quality_Score'] * 0.9:\n",
    "    print(f\"  \u2705 {best_open_source} - Best balance of quality and architecture score\")\n",
    "    print(f\"     Only {model_summary.loc[best_overall, 'Quality_Score'] - model_summary.loc[best_open_source, 'Quality_Score']:.1f}% quality drop for higher architecture score!\")\n",
    "else:\n",
    "    print(f\"  \u2705 {best_overall} - Highest quality, use if quality gap is significant\")\n",
    "    if best_open_source:\n",
    "        print(f\"  \u26a0\ufe0f  Consider {best_open_source} for higher architecture score (trade-off: {model_summary.loc[best_overall, 'Quality_Score'] - model_summary.loc[best_open_source, 'Quality_Score']:.1f}% quality)\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results\n",
    "from pathlib import Path\n",
    "\n",
    "# Using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'llm_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "model_summary.to_csv(output_dir / 'summary.csv', encoding='utf-8')\n",
    "rankings.to_csv(output_dir / 'rankings.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n\u2705 Results exported to output/llm_benchmark/:\")\n",
    "print(\"   - detailed_results.csv (all questions and answers)\")\n",
    "print(\"   - summary.csv (model averages)\")\n",
    "print(\"   - rankings.csv (final rankings)\")\n",
    "print(\"   - results.png (visualizations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Answer Comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}