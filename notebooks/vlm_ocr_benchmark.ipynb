{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision-Language Model (VLM) OCR Benchmarking\n",
    "\n",
    "Testing **Vision-Language Models** for OCR on historical SOCAR documents.\n",
    "\n",
    "## Why VLMs for OCR?\n",
    "- **Better than traditional OCR** (Tesseract, EasyOCR, etc.)\n",
    "- **Understands context** - can handle handwriting, layout, multi-language\n",
    "- **Directly processes images** - no separate OCR step needed\n",
    "- **State-of-the-art accuracy** on complex documents\n",
    "\n",
    "## Vision Models to Test:\n",
    "1. **GPT-4.1** â­â­â­â­â­ (Excellent OCR capability)\n",
    "2. **GPT-5, GPT-5-mini** â­â­â­â­â­ (Latest, best performance)\n",
    "3. **Claude-Sonnet-4.5** â­â­â­â­â­ (Very good OCR)\n",
    "4. **Phi-4-multimodal-instruct** â­â­â­â­ (Explicitly multimodal)\n",
    "5. **Llama-4-Maverick-17B** â­â­â­â­ (May have vision support)\n",
    "6. **DeepSeek-VL** (Vision-Language specialized)\n",
    "\n",
    "## Metrics:\n",
    "- **CER** (Character Error Rate) - Lower is better\n",
    "- **CSR** (Character Success Rate) = 100 - CER - Higher is better\n",
    "- **WER** (Word Error Rate) - Lower is better\n",
    "- **WSR** (Word Success Rate) = 100 - WER - Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai PyMuPDF Pillow jiwer pandas matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jiwer import wer, cer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ground truth loaded: 22386 characters\n",
      "Preview:\n",
      "XÃœLASÆ\n",
      "\n",
      "Bu tÉ™dqiqat AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) vÉ™ BakÄ± arxipelaqÄ± (BA) daxil olmaqla CÉ™nubi XÉ™zÉ™r Ã§Ã¶kÉ™kliyi sistemindÉ™ faydalÄ± qazÄ±ntÄ±larÄ±n mÉ™nÅŸÉ™yinin paleotektonik, paleocoÄŸrafi ÅŸÉ™rait vÉ™ geodinamik rejimlÉ™ necÉ™ É™laqÉ™lÉ™ndiyini, elÉ™cÉ™ dÉ™ Gec MiosendÉ™n etibarÉ™n ÆrÉ™bistan plitÉ™sinin tÉ™siri ilÉ™ formalaÅŸ...\n"
     ]
    }
   ],
   "source": [
    "def load_ground_truth(md_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load ground truth text from markdown file.\n",
    "    Removes markdown formatting for pure text comparison.\n",
    "    \"\"\"\n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Remove markdown elements\n",
    "    text = re.sub(r'^#+\\s+', '', text, flags=re.MULTILINE)  # Headers\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'---+', '', text)  # Horizontal rules\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Normalize newlines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = load_ground_truth('data/document_00.md')\n",
    "print(f\"âœ… Ground truth loaded: {len(ground_truth)} characters\")\n",
    "print(f\"Preview:\\n{ground_truth[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF to Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Converted PDF to 12 images\n",
      "First image size: (3072, 4096)\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_images(pdf_path: str, dpi: int = 150) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert PDF pages to PIL Images.\n",
    "    Higher DPI = better quality for VLMs.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        # Render at higher resolution\n",
    "        zoom = dpi / 72  # 72 DPI is default\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def image_to_base64(image: Image.Image, format: str = 'PNG') -> str:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to base64 string for API.\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=format)\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Test conversion\n",
    "pdf_path = 'data/pdfs/document_00.pdf'\n",
    "test_images = pdf_to_images(pdf_path)\n",
    "print(f\"\\nâœ… Converted PDF to {len(test_images)} images\")\n",
    "print(f\"First image size: {test_images[0].size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vision-Language Model Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured 6 vision models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    ")\n",
    "\n",
    "# Vision model configurations\n",
    "VLM_MODELS = {\n",
    "    'GPT-4.1': {\n",
    "        'deployment': 'gpt-4.1',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Excellent OCR'\n",
    "    },\n",
    "    'GPT-5': {\n",
    "        'deployment': 'gpt-5',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Latest model'\n",
    "    },\n",
    "    'GPT-5-mini': {\n",
    "        'deployment': 'gpt-5-mini',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Fast + excellent'\n",
    "    },\n",
    "    'Claude-Sonnet-4.5': {\n",
    "        'deployment': 'claude-sonnet-4-5',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Very good OCR'\n",
    "    },\n",
    "    'Phi-4-multimodal': {\n",
    "        'deployment': 'Phi-4-multimodal-instruct',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­',\n",
    "        'notes': 'Explicitly multimodal'\n",
    "    },\n",
    "    'Llama-4-Maverick-17B': {\n",
    "        'deployment': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­',\n",
    "        'notes': 'Testing vision capability'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… Configured {len(VLM_MODELS)} vision models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VLM extraction function ready\n"
     ]
    }
   ],
   "source": [
    "def vlm_extract_text(model_name: str, images: List[Image.Image], \n",
    "                     temperature: float = 0.0) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Extract text from images using Vision-Language Model.\n",
    "    Returns: (extracted_text, response_time)\n",
    "    \"\"\"\n",
    "    deployment = VLM_MODELS[model_name]['deployment']\n",
    "    \n",
    "    # OCR prompt - optimized for accuracy\n",
    "    system_prompt = \"\"\"You are an expert OCR system for historical oil & gas documents.\n",
    "\n",
    "Extract ALL text from the image with 100% accuracy. Follow these rules:\n",
    "1. Preserve EXACT spelling - including Azerbaijani, Russian, and English text\n",
    "2. Maintain original Cyrillic characters - DO NOT transliterate\n",
    "3. Keep all numbers, symbols, and special characters exactly as shown\n",
    "4. Preserve layout structure (paragraphs, line breaks)\n",
    "5. Include ALL text - headers, body, footnotes, tables, captions\n",
    "6. If text is unclear, make best effort but stay accurate\n",
    "\n",
    "Output ONLY the extracted text. No explanations, no descriptions.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        all_text = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Convert image to base64\n",
    "            image_base64 = image_to_base64(image)\n",
    "            \n",
    "            # Prepare messages with image\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"Extract all text from page {page_num}:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Call VLM with appropriate token parameter\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # GPT-5 models use max_completion_tokens, others use max_tokens\n",
    "            if deployment.startswith('gpt-5'):\n",
    "                response = azure_client.chat.completions.create(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_completion_tokens=4000\n",
    "                )\n",
    "            else:\n",
    "                response = azure_client.chat.completions.create(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=4000\n",
    "                )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            total_time += elapsed\n",
    "            \n",
    "            # Extract text\n",
    "            page_text = response.choices[0].message.content\n",
    "            all_text.append(page_text)\n",
    "            \n",
    "            print(f\"  Page {page_num}/{len(images)}: {elapsed:.1f}s\")\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = '\\n\\n'.join(all_text)\n",
    "        return full_text, total_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\", 0.0\n",
    "\n",
    "print(\"âœ… VLM extraction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metrics functions ready\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_ocr_metrics(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive OCR metrics.\n",
    "    \"\"\"\n",
    "    ref_norm = normalize_text(reference)\n",
    "    hyp_norm = normalize_text(hypothesis)\n",
    "    \n",
    "    # Character Error Rate\n",
    "    cer_score = cer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Word Error Rate\n",
    "    wer_score = wer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Success rates\n",
    "    csr_score = max(0, 100 - cer_score)\n",
    "    wsr_score = max(0, 100 - wer_score)\n",
    "    \n",
    "    # Length metrics\n",
    "    ref_chars = len(ref_norm)\n",
    "    hyp_chars = len(hyp_norm)\n",
    "    ref_words = len(ref_norm.split())\n",
    "    hyp_words = len(hyp_norm.split())\n",
    "    \n",
    "    char_length_acc = (min(ref_chars, hyp_chars) / max(ref_chars, hyp_chars) * 100) if max(ref_chars, hyp_chars) > 0 else 0\n",
    "    word_length_acc = (min(ref_words, hyp_words) / max(ref_words, hyp_words) * 100) if max(ref_words, hyp_words) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'CER': round(cer_score, 2),\n",
    "        'WER': round(wer_score, 2),\n",
    "        'CSR': round(csr_score, 2),\n",
    "        'WSR': round(wsr_score, 2),\n",
    "        'Char_Count_Ref': ref_chars,\n",
    "        'Char_Count_Hyp': hyp_chars,\n",
    "        'Word_Count_Ref': ref_words,\n",
    "        'Word_Count_Hyp': hyp_words,\n",
    "        'Char_Length_Accuracy': round(char_length_acc, 2),\n",
    "        'Word_Length_Accuracy': round(word_length_acc, 2)\n",
    "    }\n",
    "\n",
    "print(\"âœ… Metrics functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Benchmark on All VLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 6 vision models...\n",
      "This will take several minutes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select models to test\n",
    "MODELS_TO_TEST = [\n",
    "    'GPT-4.1',\n",
    "    'GPT-5',\n",
    "    'GPT-5-mini',\n",
    "    'Claude-Sonnet-4.5',\n",
    "    'Phi-4-multimodal',\n",
    "    'Llama-4-Maverick-17B',  # Added for comparison\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} vision models...\")\n",
    "print(\"This will take several minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 12 page images\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-4.1 â­â­â­â­â­\n",
      "Notes: Excellent OCR\n",
      "================================================================================\n",
      "  Page 1/12: 9.2s\n",
      "  Page 2/12: 9.5s\n",
      "  Page 3/12: 10.9s\n",
      "  Page 4/12: 10.7s\n"
     ]
    }
   ],
   "source": [
    "# Prepare PDF images\n",
    "images = pdf_to_images(pdf_path, dpi=150)\n",
    "print(f\"Prepared {len(images)} page images\\n\")\n",
    "\n",
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name} {VLM_MODELS[model_name]['rating']}\")\n",
    "    print(f\"Notes: {VLM_MODELS[model_name]['notes']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extract text\n",
    "    extracted_text, response_time = vlm_extract_text(model_name, images)\n",
    "    \n",
    "    if extracted_text.startswith('ERROR'):\n",
    "        print(f\"âŒ Failed: {extracted_text}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nâœ… Total time: {response_time:.2f}s\")\n",
    "    print(f\"âœ… Extracted: {len(extracted_text)} characters\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_ocr_metrics(ground_truth, extracted_text)\n",
    "    \n",
    "    # Store result\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Response_Time': round(response_time, 2),\n",
    "        **metrics,\n",
    "        'Rating': VLM_MODELS[model_name]['rating'],\n",
    "        'Notes': VLM_MODELS[model_name]['notes'],\n",
    "        'Extracted_Preview': extracted_text[:200]\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nðŸ“Š Metrics:\")\n",
    "    print(f\"   CSR (Character Success): {metrics['CSR']:.2f}%\")\n",
    "    print(f\"   WSR (Word Success): {metrics['WSR']:.2f}%\")\n",
    "    print(f\"   CER (Character Error): {metrics['CER']:.2f}%\")\n",
    "    print(f\"   WER (Word Error): {metrics['WER']:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… VLM OCR Benchmarking complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸ“Š VLM OCR BENCHMARKING RESULTS\n",
      "====================================================================================================\n",
      "  Model   CSR   WSR   CER   WER  Response_Time Rating\n",
      "GPT-4.1 85.86 67.61 14.14 32.39         133.49  â­â­â­â­â­\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by CSR (best first)\n",
    "df = df.sort_values('CSR', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "display_cols = ['Model', 'CSR', 'WSR', 'CER', 'WER', 'Response_Time', 'Rating']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š VLM OCR BENCHMARKING RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df[display_cols].to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('output/vlm_ocr_benchmark')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "models = df['Model'].tolist()\n",
    "colors = sns.color_palette('viridis', len(models))\n",
    "\n",
    "# 1. CSR - Character Success Rate (MAIN METRIC)\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(models, df['CSR'], color=colors)\n",
    "ax1.set_xlabel('CSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Character Success Rate (CSR)\\nðŸ† HACKATHON PRIMARY METRIC', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "for i, (model, csr) in enumerate(zip(models, df['CSR'])):\n",
    "    ax1.text(csr + 1, i, f'{csr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "ax1.axvline(x=90, color='green', linestyle='--', alpha=0.3, label='Excellent (>90%)')\n",
    "ax1.axvline(x=80, color='orange', linestyle='--', alpha=0.3, label='Good (>80%)')\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# 2. WSR - Word Success Rate\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.barh(models, df['WSR'], color=colors)\n",
    "ax2.set_xlabel('WSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Word Success Rate (WSR)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, 100)\n",
    "for i, (model, wsr) in enumerate(zip(models, df['WSR'])):\n",
    "    ax2.text(wsr + 1, i, f'{wsr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Response Time\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.barh(models, df['Response_Time'], color=colors)\n",
    "ax3.set_xlabel('Total Time (seconds) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Processing Speed', fontsize=14, fontweight='bold')\n",
    "for i, (model, time_val) in enumerate(zip(models, df['Response_Time'])):\n",
    "    ax3.text(time_val + 0.5, i, f'{time_val:.1f}s', va='center', fontsize=11)\n",
    "\n",
    "# 4. Error Rates Comparison\n",
    "ax4 = axes[1, 1]\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "ax4.bar([i - width/2 for i in x], df['CER'], width, label='CER', color='coral', alpha=0.8)\n",
    "ax4.bar([i + width/2 for i in x], df['WER'], width, label='WER', color='skyblue', alpha=0.8)\n",
    "ax4.set_ylabel('Error Rate (%) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Error Rates', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Visualization saved to '{output_dir}/results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Winner Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸ† FINAL RANKINGS\n",
      "====================================================================================================\n",
      " Rank   Model   CSR   WSR   CER   WER  Response_Time Rating\n",
      "    1 GPT-4.1 85.86 67.61 14.14 32.39         133.49  â­â­â­â­â­\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "ðŸ’¡ RECOMMENDATIONS FOR HACKATHON\n",
      "====================================================================================================\n",
      "\n",
      "ðŸ¥‡ BEST OVERALL: GPT-4.1 â­â­â­â­â­\n",
      "   CSR: 85.86% (Character Success)\n",
      "   WSR: 67.61% (Word Success)\n",
      "   CER: 14.14% (Character Error)\n",
      "   WER: 32.39% (Word Error)\n",
      "   Time: 133.49s for 12 pages\n",
      "   Notes: Excellent OCR\n",
      "\n",
      "âš¡ FASTEST: GPT-4.1\n",
      "   Time: 133.49s\n",
      "   CSR: 85.86%\n",
      "\n",
      "====================================================================================================\n",
      "ðŸ“ HACKATHON SCORING IMPACT\n",
      "====================================================================================================\n",
      "\n",
      "OCR Quality = 50% of total hackathon score\n",
      "\n",
      "Using GPT-4.1:\n",
      "  - CSR: 85.86% Ã— 50% = 42.93 points\n",
      "  - This is 85.9% accuracy on character-level OCR\n",
      "\n",
      "âš ï¸ GOOD - Consider optimizing prompt or trying other models\n",
      "\n",
      "====================================================================================================\n",
      "ðŸŽ¯ FINAL RECOMMENDATION\n",
      "====================================================================================================\n",
      "\n",
      "Use: GPT-4.1\n",
      "Reason: Highest accuracy (85.86% CSR) for hackathon OCR benchmark\n",
      "Implementation: Use vision API with same prompt as tested\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Rankings\n",
    "rankings = df[['Model', 'CSR', 'WSR', 'CER', 'WER', 'Response_Time', 'Rating']].copy()\n",
    "rankings.insert(0, 'Rank', range(1, len(rankings) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ† FINAL RANKINGS\")\n",
    "print(\"=\"*100)\n",
    "print(rankings.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Winner\n",
    "best_model = df.iloc[0]\n",
    "fastest_model = df.loc[df['Response_Time'].idxmin()]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ’¡ RECOMMENDATIONS FOR HACKATHON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nðŸ¥‡ BEST OVERALL: {best_model['Model']} {best_model['Rating']}\")\n",
    "print(f\"   CSR: {best_model['CSR']:.2f}% (Character Success)\")\n",
    "print(f\"   WSR: {best_model['WSR']:.2f}% (Word Success)\")\n",
    "print(f\"   CER: {best_model['CER']:.2f}% (Character Error)\")\n",
    "print(f\"   WER: {best_model['WER']:.2f}% (Word Error)\")\n",
    "print(f\"   Time: {best_model['Response_Time']:.2f}s for {len(images)} pages\")\n",
    "print(f\"   Notes: {best_model['Notes']}\")\n",
    "\n",
    "print(f\"\\nâš¡ FASTEST: {fastest_model['Model']}\")\n",
    "print(f\"   Time: {fastest_model['Response_Time']:.2f}s\")\n",
    "print(f\"   CSR: {fastest_model['CSR']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“ HACKATHON SCORING IMPACT\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nOCR Quality = 50% of total hackathon score\")\n",
    "print(f\"\\nUsing {best_model['Model']}:\")\n",
    "print(f\"  - CSR: {best_model['CSR']:.2f}% Ã— 50% = {best_model['CSR'] * 0.5:.2f} points\")\n",
    "print(f\"  - This is {best_model['CSR']:.1f}% accuracy on character-level OCR\")\n",
    "\n",
    "if best_model['CSR'] >= 95:\n",
    "    print(\"\\nâœ… EXCELLENT - This will score very high on OCR!\")\n",
    "elif best_model['CSR'] >= 90:\n",
    "    print(\"\\nâœ… VERY GOOD - Strong OCR performance!\")\n",
    "elif best_model['CSR'] >= 85:\n",
    "    print(\"\\nâš ï¸ GOOD - Consider optimizing prompt or trying other models\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ NEEDS IMPROVEMENT - Try other models or adjust parameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸŽ¯ FINAL RECOMMENDATION\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nUse: {best_model['Model']}\")\n",
    "print(f\"Reason: Highest accuracy ({best_model['CSR']:.2f}% CSR) for hackathon OCR benchmark\")\n",
    "print(f\"Implementation: Use vision API with same prompt as tested\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output/vlm_ocr_benchmark')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "rankings.to_csv(output_dir / 'rankings.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nâœ… Results exported to output/vlm_ocr_benchmark/:\")\n",
    "print(\"   - detailed_results.csv\")\n",
    "print(\"   - rankings.csv\")\n",
    "print(\"   - results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample Text Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸ“ SAMPLE TEXT COMPARISON (First 500 characters)\n",
      "====================================================================================================\n",
      "\n",
      "ðŸŽ¯ GROUND TRUTH:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "XÃœLASÆ\n",
      "\n",
      "Bu tÉ™dqiqat AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) vÉ™ BakÄ± arxipelaqÄ± (BA) daxil olmaqla CÉ™nubi XÉ™zÉ™r Ã§Ã¶kÉ™kliyi sistemindÉ™ faydalÄ± qazÄ±ntÄ±larÄ±n mÉ™nÅŸÉ™yinin paleotektonik, paleocoÄŸrafi ÅŸÉ™rait vÉ™ geodinamik rejimlÉ™ necÉ™ É™laqÉ™lÉ™ndiyini, elÉ™cÉ™ dÉ™ Gec MiosendÉ™n etibarÉ™n ÆrÉ™bistan plitÉ™sinin tÉ™siri ilÉ™ formalaÅŸan kollizion proseslÉ™rin bÃ¶lgÉ™nin struktur-morfoloji vÉ™ termal inkiÅŸafÄ±na nÉ™ dÉ™rÉ™cÉ™dÉ™ yÃ¶nverici rol oynadÄ±ÄŸÄ±nÄ± kompleks ÅŸÉ™kildÉ™ qiymÉ™tlÉ™ndirir. Seismotektonik gÃ¶stÉ™ricilÉ™rin, Ã§Ã¶kÃ¼ntÃ¼toplanma sÃ¼rÉ™tlÉ™ri\n",
      "\n",
      "ðŸ¤– GPT-4.1 (CSR: 85.86%):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "XÃ¼lasÉ™\n",
      "Bu mÉ™qalÉ™ ApÅŸÉ™ KÉ™r Ã§Ã¶kÉ™kliyi (AKÃ‡) vÉ™ BakÄ± arxipelaqÄ± (BA) daxil olmaqla CÉ™nubi XÉ™zÉ™r Ã§Ã¶kÉ™kliyi sistemindÉ™ faydalÄ± qazÄ±ntÄ±larÄ±n mÉ™nÅŸÉ™yinin paleotektonik, paleocoÄŸrafi ÅŸÉ™rait vÉ™ geodinamik rejim\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show comparison of first 500 characters\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“ SAMPLE TEXT COMPARISON (First 500 characters)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nðŸŽ¯ GROUND TRUTH:\")\n",
    "print(\"-\" * 100)\n",
    "print(ground_truth[:500])\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    print(f\"\\nðŸ¤– {row['Model']} (CSR: {row['CSR']:.2f}%):\")\n",
    "    print(\"-\" * 100)\n",
    "    # Get first 500 chars from extracted text\n",
    "    preview = row['Extracted_Preview'] if len(row['Extracted_Preview']) >= 500 else row['Extracted_Preview']\n",
    "    print(preview[:500])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
