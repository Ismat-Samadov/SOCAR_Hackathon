{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision-Language Model (VLM) OCR Benchmarking\n",
    "\n",
    "Testing **Vision-Language Models** for OCR on historical SOCAR documents.\n",
    "\n",
    "## Why VLMs for OCR?\n",
    "- **Better than traditional OCR** (Tesseract, EasyOCR, etc.)\n",
    "- **Understands context** - can handle handwriting, layout, multi-language\n",
    "- **Directly processes images** - no separate OCR step needed\n",
    "- **State-of-the-art accuracy** on complex documents\n",
    "\n",
    "## Vision Models to Test:\n",
    "1. **GPT-4.1** â­â­â­â­â­ (Excellent OCR capability)\n",
    "2. **GPT-5, GPT-5-mini** â­â­â­â­â­ (Latest, best performance)\n",
    "3. **Claude-Sonnet-4.5** â­â­â­â­â­ (Very good OCR)\n",
    "4. **Phi-4-multimodal-instruct** â­â­â­â­ (Explicitly multimodal)\n",
    "5. **Llama-4-Maverick-17B** â­â­â­â­ (May have vision support)\n",
    "6. **DeepSeek-VL** (Vision-Language specialized)\n",
    "\n",
    "## Metrics:\n",
    "- **CER** (Character Error Rate) - Lower is better\n",
    "- **CSR** (Character Success Rate) = 100 - CER - Higher is better\n",
    "- **WER** (Word Error Rate) - Lower is better\n",
    "- **WSR** (Word Success Rate) = 100 - WER - Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai PyMuPDF Pillow jiwer pandas matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jiwer import wer, cer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /Users/ismatsamadov/SOCAR_Hackathon\n",
      "âœ… Data directory: /Users/ismatsamadov/SOCAR_Hackathon/data\n",
      "âœ… PDFs directory: /Users/ismatsamadov/SOCAR_Hackathon/data/pdfs\n",
      "âœ… Output directory: /Users/ismatsamadov/SOCAR_Hackathon/output\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect project root (works from any directory)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('data').exists() and Path('docs').exists():\n",
    "    # Already in project root\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif Path('../data').exists() and Path('../docs').exists():\n",
    "    # In notebooks/ subdirectory\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    # Fallback: try to find project root\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'data').exists() and (current / 'docs').exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define all paths relative to project root\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "DOCS_DIR = PROJECT_ROOT / 'docs'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "PDFS_DIR = DATA_DIR / 'pdfs'\n",
    "\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Data directory: {DATA_DIR}\")\n",
    "print(f\"âœ… PDFs directory: {PDFS_DIR}\")\n",
    "print(f\"âœ… Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ground truth loaded: 22386 characters\n",
      "Preview:\n",
      "XÃœLASÆ\n",
      "\n",
      "Bu tÉ™dqiqat AÅŸaÄŸÄ± KÃ¼r Ã§Ã¶kÉ™kliyi (AKÃ‡) vÉ™ BakÄ± arxipelaqÄ± (BA) daxil olmaqla CÉ™nubi XÉ™zÉ™r Ã§Ã¶kÉ™kliyi sistemindÉ™ faydalÄ± qazÄ±ntÄ±larÄ±n mÉ™nÅŸÉ™yinin paleotektonik, paleocoÄŸrafi ÅŸÉ™rait vÉ™ geodinamik rejimlÉ™ necÉ™ É™laqÉ™lÉ™ndiyini, elÉ™cÉ™ dÉ™ Gec MiosendÉ™n etibarÉ™n ÆrÉ™bistan plitÉ™sinin tÉ™siri ilÉ™ formalaÅŸ...\n"
     ]
    }
   ],
   "source": [
    "def load_ground_truth(md_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load ground truth text from markdown file.\n",
    "    Removes markdown formatting for pure text comparison.\n",
    "    \"\"\"\n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Remove markdown elements\n",
    "    text = re.sub(r'^#+\\s+', '', text, flags=re.MULTILINE)  # Headers\n",
    "    text = re.sub(r'\\*\\*(.+?)\\*\\*', r'\\1', text)  # Bold\n",
    "    text = re.sub(r'\\*(.+?)\\*', r'\\1', text)  # Italic\n",
    "    text = re.sub(r'---+', '', text)  # Horizontal rules\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # Normalize newlines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Load ground truth - using dynamic path\n",
    "ground_truth = load_ground_truth(str(DATA_DIR / 'document_00.md'))\n",
    "print(f\"âœ… Ground truth loaded: {len(ground_truth)} characters\")\n",
    "print(f\"Preview:\\n{ground_truth[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Converted PDF to 12 images\n",
      "First image size: (3072, 4096)\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_images(pdf_path: str, dpi: int = 150) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Convert PDF pages to PIL Images.\n",
    "    Higher DPI = better quality for VLMs.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        # Render at higher resolution\n",
    "        zoom = dpi / 72  # 72 DPI is default\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def image_to_base64(image: Image.Image, format: str = 'PNG') -> str:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to base64 string for API.\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=format)\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "# Test conversion - using dynamic path\n",
    "pdf_path = str(PDFS_DIR / 'document_00.pdf')\n",
    "test_images = pdf_to_images(pdf_path)\n",
    "print(f\"\\nâœ… Converted PDF to {len(test_images)} images\")\n",
    "print(f\"First image size: {test_images[0].size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured 6 vision models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    ")\n",
    "\n",
    "# Vision model configurations\n",
    "VLM_MODELS = {\n",
    "    'GPT-4.1': {\n",
    "        'deployment': 'gpt-4.1',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Excellent OCR'\n",
    "    },\n",
    "    'GPT-5': {\n",
    "        'deployment': 'gpt-5',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Latest model'\n",
    "    },\n",
    "    'GPT-5-mini': {\n",
    "        'deployment': 'gpt-5-mini',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Fast + excellent'\n",
    "    },\n",
    "    'Claude-Sonnet-4.5': {\n",
    "        'deployment': 'claude-sonnet-4-5',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­â­',\n",
    "        'notes': 'Very good OCR'\n",
    "    },\n",
    "    'Phi-4-multimodal': {\n",
    "        'deployment': 'Phi-4-multimodal-instruct',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­',\n",
    "        'notes': 'Explicitly multimodal'\n",
    "    },\n",
    "    'Llama-4-Maverick-17B': {\n",
    "        'deployment': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
    "        'supports_vision': True,\n",
    "        'rating': 'â­â­â­â­',\n",
    "        'notes': 'Testing vision capability'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"âœ… Configured {len(VLM_MODELS)} vision models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VLM extraction function ready\n"
     ]
    }
   ],
   "source": [
    "def vlm_extract_text(model_name: str, images: List[Image.Image], \n",
    "                     temperature: float = 0.0) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Extract text from images using Vision-Language Model.\n",
    "    Returns: (extracted_text, response_time)\n",
    "    \"\"\"\n",
    "    deployment = VLM_MODELS[model_name]['deployment']\n",
    "    \n",
    "    # OCR prompt - optimized for accuracy\n",
    "    system_prompt = \"\"\"You are an expert OCR system for historical oil & gas documents.\n",
    "\n",
    "Extract ALL text from the image with 100% accuracy. Follow these rules:\n",
    "1. Preserve EXACT spelling - including Azerbaijani, Russian, and English text\n",
    "2. Maintain original Cyrillic characters - DO NOT transliterate\n",
    "3. Keep all numbers, symbols, and special characters exactly as shown\n",
    "4. Preserve layout structure (paragraphs, line breaks)\n",
    "5. Include ALL text - headers, body, footnotes, tables, captions\n",
    "6. If text is unclear, make best effort but stay accurate\n",
    "\n",
    "Output ONLY the extracted text. No explanations, no descriptions.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        all_text = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Convert image to base64\n",
    "            image_base64 = image_to_base64(image)\n",
    "            \n",
    "            # Prepare messages with image\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": f\"Extract all text from page {page_num}:\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{image_base64}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Call VLM with appropriate token parameter\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # GPT-5 models use max_completion_tokens, others use max_tokens\n",
    "            if deployment.startswith('gpt-5'):\n",
    "                response = azure_client.chat.completions.create(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_completion_tokens=4000\n",
    "                )\n",
    "            else:\n",
    "                response = azure_client.chat.completions.create(\n",
    "                    model=deployment,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=4000\n",
    "                )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            total_time += elapsed\n",
    "            \n",
    "            # Extract text\n",
    "            page_text = response.choices[0].message.content\n",
    "            all_text.append(page_text)\n",
    "            \n",
    "            print(f\"  Page {page_num}/{len(images)}: {elapsed:.1f}s\")\n",
    "        \n",
    "        # Combine all pages\n",
    "        full_text = '\\n\\n'.join(all_text)\n",
    "        return full_text, total_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\", 0.0\n",
    "\n",
    "print(\"âœ… VLM extraction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Benchmark on All VLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 6 vision models...\n",
      "This will take several minutes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select models to test\n",
    "MODELS_TO_TEST = [\n",
    "    'GPT-4.1',\n",
    "    'GPT-5',\n",
    "    'GPT-5-mini',\n",
    "    'Claude-Sonnet-4.5',\n",
    "    'Phi-4-multimodal',\n",
    "    'Llama-4-Maverick-17B',  # Added for comparison\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} vision models...\")\n",
    "print(\"This will take several minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 12 page images\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-4.1 â­â­â­â­â­\n",
      "Notes: Excellent OCR\n",
      "================================================================================\n",
      "  Page 1/12: 9.2s\n",
      "  Page 2/12: 8.8s\n",
      "  Page 3/12: 9.7s\n",
      "  Page 4/12: 10.7s\n",
      "  Page 5/12: 12.4s\n",
      "  Page 6/12: 9.9s\n",
      "  Page 7/12: 12.1s\n",
      "  Page 8/12: 10.7s\n",
      "  Page 9/12: 12.0s\n",
      "  Page 10/12: 10.9s\n",
      "  Page 11/12: 7.7s\n",
      "  Page 12/12: 6.6s\n",
      "\n",
      "âœ… Total time: 120.73s\n",
      "âœ… Extracted: 22018 characters\n",
      "\n",
      "ðŸ“Š Metrics:\n",
      "   CSR (Character Success): 86.17%\n",
      "   WSR (Word Success): 68.19%\n",
      "   CER (Character Error): 13.83%\n",
      "   WER (Word Error): 31.81%\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-5 â­â­â­â­â­\n",
      "Notes: Latest model\n",
      "================================================================================\n",
      "âŒ Failed: ERROR: Completions.create() got an unexpected keyword argument 'max_completion_tokens'\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-5-mini â­â­â­â­â­\n",
      "Notes: Fast + excellent\n",
      "================================================================================\n",
      "âŒ Failed: ERROR: Completions.create() got an unexpected keyword argument 'max_completion_tokens'\n",
      "\n",
      "================================================================================\n",
      "Testing: Claude-Sonnet-4.5 â­â­â­â­â­\n",
      "Notes: Very good OCR\n",
      "================================================================================\n",
      "âŒ Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "================================================================================\n",
      "Testing: Phi-4-multimodal â­â­â­â­\n",
      "Notes: Explicitly multimodal\n",
      "================================================================================\n",
      "  Page 1/12: 32.6s\n",
      "  Page 2/12: 3.9s\n",
      "  Page 3/12: 76.4s\n",
      "  Page 4/12: 6.0s\n",
      "âŒ Failed: ERROR: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '22657b68-c10c-4f10-b8d7-4f5d7e1d963b'}\n",
      "\n",
      "================================================================================\n",
      "Testing: Llama-4-Maverick-17B â­â­â­â­\n",
      "Notes: Testing vision capability\n",
      "================================================================================\n",
      "  Page 1/12: 6.8s\n",
      "  Page 2/12: 7.3s\n",
      "  Page 3/12: 9.0s\n",
      "  Page 4/12: 9.7s\n",
      "âŒ Failed: ERROR: Error code: 500 - {'statusCode': 500, 'message': 'Internal server error', 'activityId': '4a2cd3c9-b21e-4efd-a273-1a4a5e99e2c4'}\n",
      "\n",
      "================================================================================\n",
      "âœ… VLM OCR Benchmarking complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare PDF images\n",
    "images = pdf_to_images(pdf_path, dpi=150)\n",
    "print(f\"Prepared {len(images)} page images\\n\")\n",
    "\n",
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name} {VLM_MODELS[model_name]['rating']}\")\n",
    "    print(f\"Notes: {VLM_MODELS[model_name]['notes']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Extract text\n",
    "    extracted_text, response_time = vlm_extract_text(model_name, images)\n",
    "    \n",
    "    if extracted_text.startswith('ERROR'):\n",
    "        print(f\"âŒ Failed: {extracted_text}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nâœ… Total time: {response_time:.2f}s\")\n",
    "    print(f\"âœ… Extracted: {len(extracted_text)} characters\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_ocr_metrics(ground_truth, extracted_text)\n",
    "    \n",
    "    # Store result\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Response_Time': round(response_time, 2),\n",
    "        **metrics,\n",
    "        'Rating': VLM_MODELS[model_name]['rating'],\n",
    "        'Notes': VLM_MODELS[model_name]['notes'],\n",
    "        'Extracted_Preview': extracted_text[:200]\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nðŸ“Š Metrics:\")\n",
    "    print(f\"   CSR (Character Success): {metrics['CSR']:.2f}%\")\n",
    "    print(f\"   WSR (Word Success): {metrics['WSR']:.2f}%\")\n",
    "    print(f\"   CER (Character Error): {metrics['CER']:.2f}%\")\n",
    "    print(f\"   WER (Word Error): {metrics['WER']:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… VLM OCR Benchmarking complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ðŸ“Š VLM OCR BENCHMARKING RESULTS\n",
      "====================================================================================================\n",
      "  Model   CSR   WSR   CER   WER  Response_Time Rating\n",
      "GPT-4.1 86.17 68.19 13.83 31.81         120.73  â­â­â­â­â­\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by CSR (best first)\n",
    "df = df.sort_values('CSR', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "display_cols = ['Model', 'CSR', 'WSR', 'CER', 'WER', 'Response_Time', 'Rating']\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š VLM OCR BENCHMARKING RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df[display_cols].to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory - using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'vlm_ocr_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "models = df['Model'].tolist()\n",
    "colors = sns.color_palette('viridis', len(models))\n",
    "\n",
    "# 1. CSR - Character Success Rate (MAIN METRIC)\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(models, df['CSR'], color=colors)\n",
    "ax1.set_xlabel('CSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Character Success Rate (CSR)\\nðŸ† HACKATHON PRIMARY METRIC', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "for i, (model, csr) in enumerate(zip(models, df['CSR'])):\n",
    "    ax1.text(csr + 1, i, f'{csr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "ax1.axvline(x=90, color='green', linestyle='--', alpha=0.3, label='Excellent (>90%)')\n",
    "ax1.axvline(x=80, color='orange', linestyle='--', alpha=0.3, label='Good (>80%)')\n",
    "ax1.legend(fontsize=9)\n",
    "\n",
    "# 2. WSR - Word Success Rate\n",
    "ax2 = axes[0, 1]\n",
    "bars2 = ax2.barh(models, df['WSR'], color=colors)\n",
    "ax2.set_xlabel('WSR (%) - Higher is Better', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Word Success Rate (WSR)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim(0, 100)\n",
    "for i, (model, wsr) in enumerate(zip(models, df['WSR'])):\n",
    "    ax2.text(wsr + 1, i, f'{wsr:.2f}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Response Time\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.barh(models, df['Response_Time'], color=colors)\n",
    "ax3.set_xlabel('Total Time (seconds) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Processing Speed', fontsize=14, fontweight='bold')\n",
    "for i, (model, time_val) in enumerate(zip(models, df['Response_Time'])):\n",
    "    ax3.text(time_val + 0.5, i, f'{time_val:.1f}s', va='center', fontsize=11)\n",
    "\n",
    "# 4. Error Rates Comparison\n",
    "ax4 = axes[1, 1]\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "ax4.bar([i - width/2 for i in x], df['CER'], width, label='CER', color='coral', alpha=0.8)\n",
    "ax4.bar([i + width/2 for i in x], df['WER'], width, label='WER', color='skyblue', alpha=0.8)\n",
    "ax4.set_ylabel('Error Rate (%) - Lower is Better', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Error Rates', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Visualization saved to '{output_dir}/results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "from pathlib import Path\n",
    "\n",
    "# Using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'vlm_ocr_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "rankings.to_csv(output_dir / 'rankings.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nâœ… Results exported to output/vlm_ocr_benchmark/:\")\n",
    "print(\"   - detailed_results.csv\")\n",
    "print(\"   - rankings.csv\")\n",
    "print(\"   - results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample Text Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
