{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9146db2",
   "metadata": {},
   "source": [
    "# Vision-Language Model (VLM) OCR Benchmarking\n",
    "\n",
    "Testing **Open-Source Vision-Language Models** for OCR on historical SOCAR documents.\n",
    "\n",
    "## Why VLMs for OCR?\n",
    "- **Better than traditional OCR** (Tesseract, EasyOCR, etc.)\n",
    "- **Understands context** - can handle handwriting, layout, multi-language\n",
    "- **Directly processes images** - no separate OCR step needed\n",
    "- **State-of-the-art accuracy** on complex documents\n",
    "\n",
    "## Open-Source Vision Models to Test:\n",
    "1. **Llama-3.2-11B-Vision-Instruct** (Meta - Smaller, faster)\n",
    "2. **Llama-3.2-90B-Vision-Instruct** (Meta - Larger, better quality)\n",
    "3. **Phi-4-multimodal-instruct** (Microsoft - Explicitly multimodal)\n",
    "\n",
    "## Why Open-Source?\n",
    "- **Architecture Score**: +20% hackathon bonus for open-source stack\n",
    "- **Transparency**: Full model details and reproducibility\n",
    "- **Cost-effective**: Better long-term sustainability\n",
    "\n",
    "## Metrics:\n",
    "- **CER** (Character Error Rate) - Lower is better\n",
    "- **CSR** (Character Success Rate) = 100 - CER - Higher is better\n",
    "- **WER** (Word Error Rate) - Lower is better\n",
    "- **WSR** (Word Success Rate) = 100 - WER - Higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea02f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai PyMuPDF Pillow jiwer pandas matplotlib seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3561e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jiwer import wer, cer\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76af9b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /Users/ismatsamadov/SOCAR_Hackathon\n",
      "‚úÖ Data directory: /Users/ismatsamadov/SOCAR_Hackathon/data\n",
      "‚úÖ PDFs directory: /Users/ismatsamadov/SOCAR_Hackathon/data/pdfs\n",
      "‚úÖ Output directory: /Users/ismatsamadov/SOCAR_Hackathon/output\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect project root (works from any directory)\n",
    "if Path(\"data\").exists() and Path(\"docs\").exists():\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif Path(\"../data\").exists() and Path(\"../docs\").exists():\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / \"data\").exists() and (current / \"docs\").exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define all paths relative to project root\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "DOCS_DIR = PROJECT_ROOT / \"docs\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "PDFS_DIR = DATA_DIR / \"pdfs\"\n",
    "\n",
    "print(f\"‚úÖ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Data directory: {DATA_DIR}\")\n",
    "print(f\"‚úÖ PDFs directory: {PDFS_DIR}\")\n",
    "print(f\"‚úÖ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c09c6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ground truth loaded: 22386 characters\n",
      "Preview:\n",
      "X√úLAS∆è\n",
      "\n",
      "Bu t…ôdqiqat A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) v…ô Bakƒ± arxipelaqƒ± (BA) daxil olmaqla C…ônubi X…ôz…ôr √ß√∂k…ôkliyi sistemind…ô faydalƒ± qazƒ±ntƒ±larƒ±n m…ôn≈ü…ôyinin paleotektonik, paleocoƒürafi ≈ü…ôrait v…ô geodinamik rejiml…ô nec…ô …ôlaq…ôl…ôndiyini, el…ôc…ô d…ô Gec Miosend…ôn etibar…ôn ∆èr…ôbistan plit…ôsinin t…ôsiri il…ô formala≈ü...\n"
     ]
    }
   ],
   "source": [
    "def load_ground_truth(md_path: str) -> str:\n",
    "    \"\"\"Load ground truth text from markdown file.\"\"\"\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Remove markdown elements\n",
    "    text = re.sub(r\"^#+\\s+\", \"\", text, flags=re.MULTILINE)  # Headers\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\1\", text)  # Bold\n",
    "    text = re.sub(r\"\\*(.+?)\\*\", r\"\\1\", text)  # Italic\n",
    "    text = re.sub(r\"---+\", \"\", text)  # Horizontal rules\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", text)  # Normalize newlines\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Load ground truth using dynamic path\n",
    "ground_truth = load_ground_truth(str(DATA_DIR / \"document_00.md\"))\n",
    "print(f\"‚úÖ Ground truth loaded: {len(ground_truth)} characters\")\n",
    "print(f\"Preview:\\n{ground_truth[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81780895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Converted PDF to 12 images\n",
      "First image size: (2048, 2731)\n"
     ]
    }
   ],
   "source": [
    "def pdf_to_images(pdf_path: str, dpi: int = 100) -> List[Image.Image]:\n",
    "    \"\"\"Convert PDF pages to PIL Images with compression.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        zoom = dpi / 72  # Reduced from 150 to 100 to avoid 10MB limit\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    \n",
    "    doc.close()\n",
    "    return images\n",
    "\n",
    "def image_to_base64(image: Image.Image, format: str = \"JPEG\", quality: int = 85) -> str:\n",
    "    \"\"\"Convert PIL Image to base64 with JPEG compression to reduce size.\"\"\"\n",
    "    buffered = BytesIO()\n",
    "    # Use JPEG with quality=85 to reduce size while maintaining readability\n",
    "    image.save(buffered, format=format, quality=quality, optimize=True)\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Test conversion using dynamic path\n",
    "pdf_path = str(PDFS_DIR / \"document_00.pdf\")\n",
    "test_images = pdf_to_images(pdf_path)\n",
    "print(f\"\\n‚úÖ Converted PDF to {len(test_images)} images\")\n",
    "print(f\"First image size: {test_images[0].size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3cab7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured 3 verified working vision models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Vision model configurations - ONLY VERIFIED WORKING MODELS\n",
    "VLM_MODELS = {\n",
    "    \"GPT-4.1\": {\"deployment\": \"gpt-4.1\", \"supports_vision\": True, \"open_source\": False},\n",
    "    \"Llama-4-Maverick-17B\": {\"deployment\": \"Llama-4-Maverick-17B-128E-Instruct-FP8\", \"supports_vision\": True, \"open_source\": True},\n",
    "    \"Phi-4-multimodal\": {\"deployment\": \"Phi-4-multimodal-instruct\", \"supports_vision\": True, \"open_source\": True},\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(VLM_MODELS)} verified working vision models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67283b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VLM extraction function ready\n"
     ]
    }
   ],
   "source": [
    "def vlm_extract_text(model_name: str, images: List[Image.Image], temperature: float = 0.0) -> Tuple[str, float]:\n",
    "    \"\"\"Extract text from images using Vision-Language Model.\"\"\"\n",
    "    deployment = VLM_MODELS[model_name][\"deployment\"]\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert OCR system for historical oil & gas documents.\n",
    "\n",
    "Extract ALL text from the image with 100% accuracy. Follow these rules:\n",
    "1. Preserve EXACT spelling - including Azerbaijani, Russian, and English text\n",
    "2. Maintain original Cyrillic characters - DO NOT transliterate\n",
    "3. Keep all numbers, symbols, and special characters exactly as shown\n",
    "4. Preserve layout structure (paragraphs, line breaks)\n",
    "5. Include ALL text - headers, body, footnotes, tables, captions\n",
    "\n",
    "Output ONLY the extracted text. No explanations, no descriptions.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        all_text = []\n",
    "        total_time = 0\n",
    "        \n",
    "        for page_num, image in enumerate(images, 1):\n",
    "            # Use JPEG compression to reduce image size\n",
    "            image_base64 = image_to_base64(image, format=\"JPEG\", quality=85)\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": f\"Extract all text from page {page_num}:\"},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Use max_tokens for all models (simpler and more compatible)\n",
    "            response = azure_client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=4000\n",
    "            )\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            total_time += elapsed\n",
    "            \n",
    "            page_text = response.choices[0].message.content\n",
    "            all_text.append(page_text)\n",
    "            print(f\"  Page {page_num}/{len(images)}: {elapsed:.1f}s\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(all_text)\n",
    "        return full_text, total_time\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        # Provide helpful error messages\n",
    "        if \"max_tokens\" in error_msg.lower() or \"max_completion_tokens\" in error_msg.lower():\n",
    "            return f\"ERROR: Token parameter issue - {error_msg}\", 0.0\n",
    "        elif \"deployment\" in error_msg.lower() or \"not found\" in error_msg.lower():\n",
    "            return f\"ERROR: Model deployment not found - {deployment}\", 0.0\n",
    "        else:\n",
    "            return f\"ERROR: {error_msg}\", 0.0\n",
    "\n",
    "print(\"‚úÖ VLM extraction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "553f24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metrics calculation function ready\n"
     ]
    }
   ],
   "source": [
    "def calculate_ocr_metrics(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Calculate OCR accuracy metrics.\"\"\"\n",
    "    # Normalize text\n",
    "    ref_clean = reference.lower().strip()\n",
    "    hyp_clean = hypothesis.lower().strip()\n",
    "    \n",
    "    # Calculate error rates\n",
    "    cer_score = cer(ref_clean, hyp_clean) * 100\n",
    "    wer_score = wer(ref_clean, hyp_clean) * 100\n",
    "    \n",
    "    # Calculate success rates\n",
    "    csr = max(0, 100 - cer_score)\n",
    "    wsr = max(0, 100 - wer_score)\n",
    "    \n",
    "    return {\n",
    "        \"CER\": round(cer_score, 2),\n",
    "        \"WER\": round(wer_score, 2),\n",
    "        \"CSR\": round(csr, 2),\n",
    "        \"WSR\": round(wsr, 2)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metrics calculation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb965dc",
   "metadata": {},
   "source": [
    "## Run VLM Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6da3824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3 vision models...\n",
      "Models ordered: potentially failing/slow first, known good last\n",
      "This allows early cancellation if issues occur.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select models to test - ORDERED: Test potentially failing/slow models FIRST\n",
    "MODELS_TO_TEST = [\n",
    "    # Add new untested models here at the top (fail-fast approach)\n",
    "    # \"New-Model-Name\",\n",
    "    \n",
    "    # Known slow model (test early so we can cancel if needed)\n",
    "    \"Phi-4-multimodal\",     # 27.64% CSR, very slow (~552s for 12 pages)\n",
    "    \n",
    "    # Best performing models (test last since we know they work)\n",
    "    \"GPT-4.1\",              # 88.32% CSR, 107s - Best accuracy\n",
    "    \"Llama-4-Maverick-17B\", # 87.60% CSR, 79s - Best open-source\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} vision models...\")\n",
    "print(\"Models ordered: potentially failing/slow first, known good last\")\n",
    "print(\"This allows early cancellation if issues occur.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba0ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 12 page images\n",
      "\n",
      "********************************\n",
      "Testing: Phi-4-multimodal\n",
      "********************************\n",
      "  Page 1/12: 32.3s\n",
      "  Page 2/12: 33.5s\n",
      "  Page 3/12: 33.2s\n",
      "  Page 4/12: 37.5s\n",
      "  Page 5/12: 59.6s\n",
      "  Page 6/12: 59.4s\n",
      "  Page 7/12: 35.5s\n",
      "  Page 8/12: 38.1s\n",
      "  Page 9/12: 242.6s\n"
     ]
    }
   ],
   "source": [
    "# Prepare PDF images\n",
    "images = pdf_to_images(pdf_path, dpi=100)\n",
    "print(f\"Prepared {len(images)} page images\\n\")\n",
    "\n",
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(\"********************************\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(\"********************************\")\n",
    "    \n",
    "    # Extract text\n",
    "    extracted_text, response_time = vlm_extract_text(model_name, images)\n",
    "    \n",
    "    if extracted_text.startswith(\"ERROR\"):\n",
    "        print(f\"‚ùå Failed: {extracted_text}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total time: {response_time:.2f}s\")\n",
    "    print(f\"‚úÖ Extracted: {len(extracted_text)} characters\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_ocr_metrics(ground_truth, extracted_text)\n",
    "    \n",
    "    # Store result\n",
    "    result = {\n",
    "        \"Model\": model_name,\n",
    "        \"Response_Time\": round(response_time, 2),\n",
    "        **metrics\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Show summary\n",
    "    print(f\"\\nüìä Metrics:\")\n",
    "    print(f\"   CSR (Character Success): {metrics['CSR']:.2f}%\")\n",
    "    print(f\"   WSR (Word Success): {metrics['WSR']:.2f}%\")\n",
    "\n",
    "print(\"\\n********************************\")\n",
    "print(\"‚úÖ VLM OCR Benchmarking complete!\")\n",
    "print(\"********************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bcdcc9",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä VLM OCR BENCHMARKING RESULTS\n",
      "====================================================================================================\n",
      "               Model  Response_Time   CER   WER   CSR   WSR\n",
      "             GPT-4.1         199.18 11.88 32.56 88.12 67.44\n",
      "Llama-4-Maverick-17B          75.08 12.25 38.09 87.75 61.91\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by CSR (best first)\n",
    "df = df.sort_values(\"CSR\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä VLM OCR BENCHMARKING RESULTS\")\n",
    "print(\"=\"*100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5452c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PRESENTATION-QUALITY visualizations\n",
    "output_dir = OUTPUT_DIR / \"vlm_ocr_benchmark\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Enhanced color palette\n",
    "colors = {\n",
    "    \"GPT-4.1\": \"#FF6B6B\",  # Red - Premium model\n",
    "    \"Llama-4-Maverick-17B\": \"#4ECDC4\",  # Teal - Open-source champion\n",
    "    # \"Phi-4-multimodal\": \"#95E1D3\",  # Light teal\n",
    "}\n",
    "model_colors = [colors.get(m, \"#999999\") for m in df[\"Model\"]]\n",
    "\n",
    "# Create comprehensive figure\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "models = df[\"Model\"].tolist()\n",
    "\n",
    "# === 1. Main Accuracy Comparison (Large, Top Left) ===\n",
    "ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
    "x_pos = range(len(models))\n",
    "bars = ax1.barh(x_pos, df[\"CSR\"], color=model_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_yticks(x_pos)\n",
    "ax1.set_yticklabels(models, fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(\"Character Success Rate (%)\", fontsize=16, fontweight='bold')\n",
    "ax1.set_title(\"OCR Accuracy Comparison\\n(Higher is Better)\", fontsize=18, fontweight='bold', pad=20)\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.axvline(x=80, color='green', linestyle='--', alpha=0.3, linewidth=2, label='Good (80%)')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, (model, csr) in enumerate(zip(models, df[\"CSR\"])):\n",
    "    ax1.text(csr + 2, i, f\"{csr:.2f}%\", va=\"center\", fontsize=13, fontweight='bold')\n",
    "# Add best/worst markers\n",
    "best_idx = df[\"CSR\"].idxmax()\n",
    "ax1.text(df.loc[best_idx, \"CSR\"] - 8, best_idx, \"‚òÖ BEST\", \n",
    "         va=\"center\", ha=\"right\", fontsize=12, color='gold', fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='darkred', alpha=0.8))\n",
    "ax1.legend(fontsize=11, loc='lower right')\n",
    "\n",
    "# === 2. Speed Comparison (Top Right) ===\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "bars = ax2.barh(range(len(models)), df[\"Response_Time\"], color=model_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.set_yticks(range(len(models)))\n",
    "ax2.set_yticklabels(models, fontsize=11)\n",
    "ax2.set_xlabel(\"Time (seconds)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Processing Speed\\n(Lower is Better)\", fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "# Add time labels\n",
    "for i, (model, time_val) in enumerate(zip(models, df[\"Response_Time\"])):\n",
    "    ax2.text(time_val + max(df[\"Response_Time\"])*0.02, i, f\"{time_val:.1f}s\", \n",
    "             va=\"center\", fontsize=10, fontweight='bold')\n",
    "# Mark fastest\n",
    "fastest_idx = df[\"Response_Time\"].idxmin()\n",
    "ax2.text(df.loc[fastest_idx, \"Response_Time\"] - 5, fastest_idx, \"‚ö°\", \n",
    "         va=\"center\", ha=\"right\", fontsize=16)\n",
    "\n",
    "# === 3. Word Success Rate (Middle Right) ===\n",
    "ax3 = fig.add_subplot(gs[1, 2])\n",
    "bars = ax3.barh(range(len(models)), df[\"WSR\"], color=model_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.set_yticks(range(len(models)))\n",
    "ax3.set_yticklabels(models, fontsize=11)\n",
    "ax3.set_xlabel(\"WSR (%)\", fontsize=12, fontweight='bold')\n",
    "ax3.set_title(\"Word Success Rate\", fontsize=14, fontweight='bold')\n",
    "ax3.set_xlim(0, 100)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (model, wsr) in enumerate(zip(models, df[\"WSR\"])):\n",
    "    ax3.text(wsr + 2, i, f\"{wsr:.1f}%\", va=\"center\", fontsize=10, fontweight='bold')\n",
    "\n",
    "# === 4. Error Rates Comparison (Bottom Left) ===\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "x = range(len(models))\n",
    "width = 0.35\n",
    "ax4.bar([i - width/2 for i in x], df[\"CER\"], width, label=\"CER (Character)\", \n",
    "        color=\"#FF6B6B\", alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.bar([i + width/2 for i in x], df[\"WER\"], width, label=\"WER (Word)\", \n",
    "        color=\"#4ECDC4\", alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel(\"Error Rate (%) - Lower is Better\", fontsize=12, fontweight='bold')\n",
    "ax4.set_title(\"Error Rate Breakdown\", fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(models, rotation=20, ha=\"right\", fontsize=11)\n",
    "ax4.legend(fontsize=11, loc='upper right')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# === 5. Accuracy vs Speed Scatter (Bottom Middle) ===\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "for i, model in enumerate(models):\n",
    "    ax5.scatter(df.loc[i, \"Response_Time\"], df.loc[i, \"CSR\"], \n",
    "               s=400, color=model_colors[i], alpha=0.7, \n",
    "               edgecolor='black', linewidth=2, zorder=3)\n",
    "    ax5.annotate(model, (df.loc[i, \"Response_Time\"], df.loc[i, \"CSR\"]), \n",
    "                fontsize=9, fontweight='bold', ha='center', va='bottom',\n",
    "                xytext=(0, 8), textcoords='offset points')\n",
    "ax5.set_xlabel(\"Processing Time (seconds)\", fontsize=12, fontweight='bold')\n",
    "ax5.set_ylabel(\"Character Success Rate (%)\", fontsize=12, fontweight='bold')\n",
    "ax5.set_title(\"Accuracy vs Speed Trade-off\", fontsize=14, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.axhline(y=80, color='green', linestyle='--', alpha=0.3, linewidth=2)\n",
    "# Add quadrant labels\n",
    "ax5.text(0.98, 0.98, \"Ideal Zone\\n(Fast + Accurate)\", transform=ax5.transAxes,\n",
    "        fontsize=10, va='top', ha='right', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# === 6. Model Ranking Summary (Bottom Right) ===\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.axis('off')\n",
    "ranking_text = \"üèÜ MODEL RANKINGS\\n\" + \"=\"*35 + \"\\n\\n\"\n",
    "for i, row in df.iterrows():\n",
    "    rank_emoji = [\"ü•á\", \"ü•à\", \"ü•â\"][i] if i < 3 else f\"{i+1}.\"\n",
    "    model_type = \"üîì Open\" if row[\"Model\"] == \"Llama-4-Maverick-17B\" else \"üîí Closed\"\n",
    "    ranking_text += f\"{rank_emoji} {row['Model']}\\n\"\n",
    "    ranking_text += f\"   {model_type} | CSR: {row['CSR']:.2f}%\\n\"\n",
    "    ranking_text += f\"   Time: {row['Response_Time']:.1f}s\\n\\n\"\n",
    "\n",
    "# Add hackathon scoring estimate\n",
    "best_csr = df[\"CSR\"].max()\n",
    "ocr_score = (best_csr / 100) * 500  # 500 points max for OCR\n",
    "ranking_text += \"=\"*35 + \"\\n\"\n",
    "ranking_text += f\"üìä HACKATHON ESTIMATE:\\n\"\n",
    "ranking_text += f\"   OCR Score: {ocr_score:.1f}/500 pts\\n\"\n",
    "ranking_text += f\"   Based on {best_csr:.2f}% CSR\"\n",
    "\n",
    "ax6.text(0.05, 0.95, ranking_text, transform=ax6.transAxes,\n",
    "        fontsize=11, va='top', ha='left', family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8, pad=1))\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('VLM OCR Benchmark Results - SOCAR Historical Documents', \n",
    "            fontsize=22, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig(output_dir / \"results.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.savefig(output_dir / \"results_presentation.png\", dpi=150, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Presentation-quality visualizations saved:\")\n",
    "print(f\"   üìä {output_dir}/results.png (High-res: 300 DPI)\")\n",
    "print(f\"   üìä {output_dir}/results_presentation.png (Web-optimized: 150 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results using dynamic path\n",
    "output_dir = OUTPUT_DIR / \"vlm_ocr_benchmark\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_dir / \"detailed_results.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n‚úÖ Results exported to output/vlm_ocr_benchmark/:\")\n",
    "print(\"   - detailed_results.csv\")\n",
    "print(\"   - results.png\")\n",
    "print(\"   - results_presentation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42rogqo6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE INDIVIDUAL PRESENTATION SLIDES ===\n",
    "# These are standalone charts perfect for PowerPoint/Google Slides\n",
    "\n",
    "# Color palette\n",
    "colors_map = {\n",
    "    \"GPT-4.1\": \"#FF6B6B\",\n",
    "    \"Llama-4-Maverick-17B\": \"#4ECDC4\",\n",
    "    \"Phi-4-multimodal\": \"#95E1D3\",\n",
    "}\n",
    "\n",
    "# === SLIDE 1: Main Accuracy Chart (Clean, Large) ===\n",
    "fig1, ax = plt.subplots(figsize=(12, 6))\n",
    "model_colors = [colors_map.get(m, \"#999999\") for m in df[\"Model\"]]\n",
    "y_pos = range(len(df[\"Model\"]))\n",
    "\n",
    "bars = ax.barh(y_pos, df[\"CSR\"], color=model_colors, alpha=0.85, edgecolor='black', linewidth=2.5, height=0.6)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df[\"Model\"], fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel(\"Character Success Rate (%)\", fontsize=18, fontweight='bold')\n",
    "ax.set_title(\"VLM OCR Accuracy Comparison\", fontsize=22, fontweight='bold', pad=20)\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add reference line\n",
    "ax.axvline(x=85, color='green', linestyle='--', alpha=0.4, linewidth=2.5, label='Target (85%)')\n",
    "\n",
    "# Add value labels with better positioning\n",
    "for i, (model, csr) in enumerate(zip(df[\"Model\"], df[\"CSR\"])):\n",
    "    ax.text(csr + 1.5, i, f\"{csr:.1f}%\", va=\"center\", fontsize=15, fontweight='bold')\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = df[\"CSR\"].idxmax()\n",
    "ax.scatter([df.loc[best_idx, \"CSR\"]], [best_idx], s=800, color='gold', marker='*', \n",
    "           edgecolor='darkred', linewidth=3, zorder=5, label='Best Model')\n",
    "\n",
    "ax.legend(fontsize=14, loc='lower right', framealpha=0.95)\n",
    "ax.grid(axis='x', alpha=0.25, linewidth=1.5)\n",
    "ax.set_facecolor('#F8F9FA')\n",
    "fig1.patch.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"slide_1_accuracy.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Slide 1: Accuracy comparison saved\")\n",
    "\n",
    "# === SLIDE 2: Speed vs Accuracy Scatter ===\n",
    "fig2, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    color = colors_map.get(row[\"Model\"], \"#999999\")\n",
    "    # Larger markers for better visibility\n",
    "    ax.scatter(row[\"Response_Time\"], row[\"CSR\"], s=800, color=color, alpha=0.7,\n",
    "              edgecolor='black', linewidth=3, zorder=3)\n",
    "    \n",
    "    # Model name labels with background\n",
    "    ax.annotate(row[\"Model\"], (row[\"Response_Time\"], row[\"CSR\"]), \n",
    "               fontsize=13, fontweight='bold', ha='center', va='bottom',\n",
    "               xytext=(0, 12), textcoords='offset points',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='gray', alpha=0.9))\n",
    "\n",
    "ax.set_xlabel(\"Processing Time (seconds) - Lower is Better\", fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel(\"Character Success Rate (%) - Higher is Better\", fontsize=18, fontweight='bold')\n",
    "ax.set_title(\"OCR Accuracy vs Processing Speed Trade-off\", fontsize=22, fontweight='bold', pad=20)\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=85, color='green', linestyle='--', alpha=0.4, linewidth=2.5, label='Target Accuracy (85%)')\n",
    "ax.axvline(x=100, color='orange', linestyle='--', alpha=0.4, linewidth=2.5, label='Target Speed (100s)')\n",
    "\n",
    "# Ideal zone annotation\n",
    "ideal_x = ax.get_xlim()[0] + (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.05\n",
    "ideal_y = ax.get_ylim()[1] - (ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.05\n",
    "ax.text(ideal_x, ideal_y, \"‚òÖ IDEAL ZONE ‚òÖ\\n(Fast & Accurate)\", \n",
    "       fontsize=14, fontweight='bold', va='top', ha='left',\n",
    "       bbox=dict(boxstyle='round,pad=1', facecolor='lightgreen', edgecolor='darkgreen', \n",
    "                linewidth=2, alpha=0.7))\n",
    "\n",
    "ax.legend(fontsize=14, loc='lower right', framealpha=0.95)\n",
    "ax.grid(True, alpha=0.25, linewidth=1.5)\n",
    "ax.set_facecolor('#F8F9FA')\n",
    "fig2.patch.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"slide_2_speed_vs_accuracy.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Slide 2: Speed vs Accuracy saved\")\n",
    "\n",
    "# === SLIDE 3: Error Rate Comparison ===\n",
    "fig3, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(df[\"Model\"]))\n",
    "width = 0.4\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], df[\"CER\"], width, label=\"Character Error Rate (CER)\", \n",
    "              color=\"#FF6B6B\", alpha=0.8, edgecolor='black', linewidth=2)\n",
    "bars2 = ax.bar([i + width/2 for i in x], df[\"WER\"], width, label=\"Word Error Rate (WER)\", \n",
    "              color=\"#4ECDC4\", alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel(\"Error Rate (%) - Lower is Better\", fontsize=18, fontweight='bold')\n",
    "ax.set_title(\"OCR Error Rates by Model\", fontsize=22, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df[\"Model\"], fontsize=14, fontweight='bold', rotation=15, ha='right')\n",
    "ax.legend(fontsize=15, loc='upper right', framealpha=0.95)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (cer, wer) in enumerate(zip(df[\"CER\"], df[\"WER\"])):\n",
    "    ax.text(i - width/2, cer + 1, f\"{cer:.1f}%\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    ax.text(i + width/2, wer + 1, f\"{wer:.1f}%\", ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.grid(axis='y', alpha=0.25, linewidth=1.5)\n",
    "ax.set_facecolor('#F8F9FA')\n",
    "fig3.patch.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"slide_3_error_rates.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Slide 3: Error rates saved\")\n",
    "\n",
    "# === SLIDE 4: Model Comparison Table ===\n",
    "fig4, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Prepare table data\n",
    "table_data = []\n",
    "table_data.append([\"Rank\", \"Model\", \"Type\", \"CSR (%)\", \"WSR (%)\", \"Time (s)\", \"Speed\"])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    rank = [\"ü•á\", \"ü•à\", \"ü•â\"][i] if i < 3 else f\"{i+1}\"\n",
    "    model_type = \"Open-Source\" if row[\"Model\"] == \"Llama-4-Maverick-17B\" else \"Closed\"\n",
    "    speed_rating = \"‚ö°‚ö°‚ö°\" if row[\"Response_Time\"] < 100 else \"‚ö°‚ö°\" if row[\"Response_Time\"] < 200 else \"‚ö°\"\n",
    "    \n",
    "    table_data.append([\n",
    "        rank,\n",
    "        row[\"Model\"],\n",
    "        model_type,\n",
    "        f\"{row['CSR']:.2f}\",\n",
    "        f\"{row['WSR']:.2f}\",\n",
    "        f\"{row['Response_Time']:.1f}\",\n",
    "        speed_rating\n",
    "    ])\n",
    "\n",
    "# Add hackathon score estimate\n",
    "best_csr = df[\"CSR\"].max()\n",
    "ocr_score = (best_csr / 100) * 500\n",
    "table_data.append([\"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "table_data.append([\"üìä\", \"HACKATHON ESTIMATE\", f\"OCR Score: {ocr_score:.1f}/500\", \"\", \"\", \"\", \"\"])\n",
    "\n",
    "# Create table\n",
    "table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                colWidths=[0.08, 0.28, 0.15, 0.12, 0.12, 0.12, 0.13])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(13)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header row\n",
    "for i in range(7):\n",
    "    cell = table[(0, i)]\n",
    "    cell.set_facecolor('#2C3E50')\n",
    "    cell.set_text_props(weight='bold', color='white', fontsize=14)\n",
    "\n",
    "# Style data rows with alternating colors\n",
    "for i in range(1, len(table_data)-2):\n",
    "    for j in range(7):\n",
    "        cell = table[(i, j)]\n",
    "        if i % 2 == 0:\n",
    "            cell.set_facecolor('#ECF0F1')\n",
    "        else:\n",
    "            cell.set_facecolor('white')\n",
    "        cell.set_text_props(fontsize=13)\n",
    "        \n",
    "        # Bold model names\n",
    "        if j == 1:\n",
    "            cell.set_text_props(weight='bold', fontsize=13)\n",
    "\n",
    "# Style summary row\n",
    "for j in range(7):\n",
    "    cell = table[(len(table_data)-1, j)]\n",
    "    cell.set_facecolor('#FFF3CD')\n",
    "    cell.set_text_props(weight='bold', fontsize=13)\n",
    "\n",
    "# Add title\n",
    "fig4.suptitle('VLM OCR Benchmark Summary Table', fontsize=22, fontweight='bold', y=0.98)\n",
    "fig4.patch.set_facecolor('white')\n",
    "\n",
    "plt.savefig(output_dir / \"slide_4_summary_table.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Slide 4: Summary table saved\")\n",
    "\n",
    "# === SLIDE 5: Success Rates Side-by-Side ===\n",
    "fig5, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Character Success Rate\n",
    "model_colors = [colors_map.get(m, \"#999999\") for m in df[\"Model\"]]\n",
    "y_pos = range(len(df[\"Model\"]))\n",
    "\n",
    "bars1 = ax1.barh(y_pos, df[\"CSR\"], color=model_colors, alpha=0.85, edgecolor='black', linewidth=2, height=0.6)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(df[\"Model\"], fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(\"Character Success Rate (%)\", fontsize=15, fontweight='bold')\n",
    "ax1.set_title(\"Character-Level Accuracy\", fontsize=18, fontweight='bold', pad=15)\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.axvline(x=85, color='green', linestyle='--', alpha=0.4, linewidth=2)\n",
    "ax1.grid(axis='x', alpha=0.25)\n",
    "ax1.set_facecolor('#F8F9FA')\n",
    "\n",
    "for i, csr in enumerate(df[\"CSR\"]):\n",
    "    ax1.text(csr + 1.5, i, f\"{csr:.1f}%\", va=\"center\", fontsize=13, fontweight='bold')\n",
    "\n",
    "# Word Success Rate\n",
    "bars2 = ax2.barh(y_pos, df[\"WSR\"], color=model_colors, alpha=0.85, edgecolor='black', linewidth=2, height=0.6)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(df[\"Model\"], fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel(\"Word Success Rate (%)\", fontsize=15, fontweight='bold')\n",
    "ax2.set_title(\"Word-Level Accuracy\", fontsize=18, fontweight='bold', pad=15)\n",
    "ax2.set_xlim(0, 100)\n",
    "ax2.axvline(x=60, color='orange', linestyle='--', alpha=0.4, linewidth=2)\n",
    "ax2.grid(axis='x', alpha=0.25)\n",
    "ax2.set_facecolor('#F8F9FA')\n",
    "\n",
    "for i, wsr in enumerate(df[\"WSR\"]):\n",
    "    ax2.text(wsr + 1.5, i, f\"{wsr:.1f}%\", va=\"center\", fontsize=13, fontweight='bold')\n",
    "\n",
    "fig5.suptitle('OCR Success Rates Comparison', fontsize=22, fontweight='bold', y=1.00)\n",
    "fig5.patch.set_facecolor('white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"slide_5_success_rates.png\", dpi=300, bbox_inches=\"tight\", facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Slide 5: Success rates comparison saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üé® ALL PRESENTATION SLIDES GENERATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìÅ Location: {output_dir}/\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. slide_1_accuracy.png - Main accuracy chart\")\n",
    "print(\"  2. slide_2_speed_vs_accuracy.png - Trade-off analysis\")\n",
    "print(\"  3. slide_3_error_rates.png - Error rate comparison\")\n",
    "print(\"  4. slide_4_summary_table.png - Complete results table\")\n",
    "print(\"  5. slide_5_success_rates.png - CSR vs WSR comparison\")\n",
    "print(\"  6. results.png - Comprehensive dashboard (6-panel)\")\n",
    "print(\"  7. results_presentation.png - Web-optimized dashboard\")\n",
    "print(\"\\nAll charts are high-resolution (300 DPI) and ready for presentations!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
