{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Optimization Benchmark\n",
    "\n",
    "**Comprehensive testing of ALL RAG components to maximize LLM Judge score**\n",
    "\n",
    "## What We're Testing:\n",
    "\n",
    "### 1. Embedding Models (Vector Representations)\n",
    "- `BAAI/bge-large-en-v1.5` (Current - 1024 dim, best quality)\n",
    "- `BAAI/bge-base-en-v1.5` (768 dim, faster)\n",
    "- `intfloat/multilingual-e5-large` (1024 dim, multi-language)\n",
    "- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` (768 dim, multilingual)\n",
    "- `sentence-transformers/all-MiniLM-L6-v2` (384 dim, very fast)\n",
    "\n",
    "### 2. Retrieval Strategies\n",
    "- **Top-K**: Test 1, 3, 5, 10 documents\n",
    "- **MMR** (Maximal Marginal Relevance): Diversity vs relevance trade-off\n",
    "- **Similarity Threshold**: Filter low-relevance docs\n",
    "- **Reranking**: Use cross-encoder to rerank results\n",
    "\n",
    "### 3. Chunking Strategies (Already in Vector DB, but we'll compare)\n",
    "- Chunk size: 256, 512, 600 (current), 1000 tokens\n",
    "- Overlap: 0, 50, 100 (current), 200 chars\n",
    "\n",
    "### 4. LLM Models\n",
    "- Llama-4-Maverick-17B (open-source)\n",
    "- DeepSeek-R1 (reasoning)\n",
    "- GPT-4.1, GPT-5, GPT-5-mini\n",
    "- Claude-Sonnet-4.5\n",
    "\n",
    "### 5. Prompting Techniques\n",
    "- **Baseline**: Simple context + question\n",
    "- **Citation-focused**: Emphasize source references\n",
    "- **Step-by-step**: Chain-of-thought reasoning\n",
    "- **Few-shot**: Include example Q&A\n",
    "\n",
    "## LLM Judge Evaluation Criteria:\n",
    "- **Accuracy** (35%): Answer correctness\n",
    "- **Relevance** (35%): Citation quality and relevance\n",
    "- **Completeness** (30%): Thorough answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pinecone-client sentence-transformers rank-bm25 python-dotenv pandas matplotlib seaborn jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/SOCAR_Hackathon/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import AzureOpenAI\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from jiwer import wer, cer\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /Users/ismatsamadov/SOCAR_Hackathon\n",
      "‚úÖ Docs directory: /Users/ismatsamadov/SOCAR_Hackathon/docs\n",
      "‚úÖ Output directory: /Users/ismatsamadov/SOCAR_Hackathon/output\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect project root (works from any directory)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('data').exists() and Path('docs').exists():\n",
    "    # Already in project root\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "elif Path('../data').exists() and Path('../docs').exists():\n",
    "    # In notebooks/ subdirectory\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "else:\n",
    "    # Fallback: try to find project root\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / 'data').exists() and (current / 'docs').exists():\n",
    "            PROJECT_ROOT = current\n",
    "            break\n",
    "        current = current.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Define all paths relative to project root\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "DOCS_DIR = PROJECT_ROOT / 'docs'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'output'\n",
    "\n",
    "print(f\"‚úÖ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Docs directory: {DOCS_DIR}\")\n",
    "print(f\"‚úÖ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 5 test questions\n",
      "  - Example1\n",
      "  - Example2\n",
      "  - Example3\n",
      "  - Example4\n",
      "  - Example5\n"
     ]
    }
   ],
   "source": [
    "# Load test cases - using dynamic paths\n",
    "with open(DOCS_DIR / 'sample_questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open(DOCS_DIR / 'sample_answers.json', 'r', encoding='utf-8') as f:\n",
    "    expected_answers = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(questions)} test questions\")\n",
    "for key in questions.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "index = pc.Index(os.getenv('PINECONE_INDEX_NAME', 'hackathon'))\n",
    "\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"‚úÖ Vector DB connected\")\n",
    "print(f\"   Total vectors: {stats['total_vector_count']}\")\n",
    "print(f\"   Dimensions: {stats['dimension']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding Models Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = {\n",
    "    'bge-large-en': {\n",
    "        'name': 'BAAI/bge-large-en-v1.5',\n",
    "        'dimensions': 1024,\n",
    "        'notes': 'Current model - best quality'\n",
    "    },\n",
    "    'bge-base-en': {\n",
    "        'name': 'BAAI/bge-base-en-v1.5',\n",
    "        'dimensions': 768,\n",
    "        'notes': 'Faster, slightly lower quality'\n",
    "    },\n",
    "    'multilingual-e5-large': {\n",
    "        'name': 'intfloat/multilingual-e5-large',\n",
    "        'dimensions': 1024,\n",
    "        'notes': 'Multi-language optimized'\n",
    "    },\n",
    "    'paraphrase-multilingual': {\n",
    "        'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "        'dimensions': 768,\n",
    "        'notes': 'Good for Azerbaijani/Russian'\n",
    "    },\n",
    "    'all-MiniLM-L6': {\n",
    "        'name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'dimensions': 384,\n",
    "        'notes': 'Very fast, lower quality'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load embedding models (only test 1024-dim models for existing Pinecone index)\n",
    "EMBEDDING_MODELS_TO_TEST = [\n",
    "    'bge-large-en',  # Current\n",
    "    'multilingual-e5-large',  # Alternative with same dims\n",
    "]\n",
    "\n",
    "embedding_cache = {}\n",
    "\n",
    "for model_key in EMBEDDING_MODELS_TO_TEST:\n",
    "    model_name = EMBEDDING_MODELS[model_key]['name']\n",
    "    print(f\"Loading {model_key}...\")\n",
    "    embedding_cache[model_key] = SentenceTransformer(model_name)\n",
    "    print(f\"  ‚úÖ {model_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(embedding_cache)} embedding models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_vanilla(query: str, embed_model: SentenceTransformer, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Vanilla retrieval: Simple top-k vector search.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    \n",
    "    documents = []\n",
    "    for match in results['matches']:\n",
    "        documents.append({\n",
    "            'pdf_name': match['metadata'].get('pdf_name', 'unknown.pdf'),\n",
    "            'page_number': match['metadata'].get('page_number', 0),\n",
    "            'content': match['metadata'].get('text', ''),\n",
    "            'score': match.get('score', 0.0)\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def retrieve_with_threshold(query: str, embed_model: SentenceTransformer, \n",
    "                           top_k: int = 10, threshold: float = 0.7) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve with similarity threshold filtering.\n",
    "    \"\"\"\n",
    "    docs = retrieve_vanilla(query, embed_model, top_k=top_k)\n",
    "    return [doc for doc in docs if doc['score'] >= threshold]\n",
    "\n",
    "\n",
    "def retrieve_with_mmr(query: str, embed_model: SentenceTransformer, \n",
    "                     top_k: int = 3, lambda_param: float = 0.5, fetch_k: int = 20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    MMR (Maximal Marginal Relevance) for diversity.\n",
    "    lambda=1 ‚Üí pure relevance, lambda=0 ‚Üí pure diversity\n",
    "    \"\"\"\n",
    "    # Fetch more candidates\n",
    "    candidates = retrieve_vanilla(query, embed_model, top_k=fetch_k)\n",
    "    \n",
    "    if len(candidates) <= top_k:\n",
    "        return candidates[:top_k]\n",
    "    \n",
    "    # Query embedding\n",
    "    query_emb = embed_model.encode(query)\n",
    "    \n",
    "    # Get embeddings for candidates\n",
    "    candidate_texts = [doc['content'] for doc in candidates]\n",
    "    candidate_embs = embed_model.encode(candidate_texts)\n",
    "    \n",
    "    # MMR algorithm\n",
    "    selected = []\n",
    "    selected_embs = []\n",
    "    \n",
    "    for _ in range(min(top_k, len(candidates))):\n",
    "        mmr_scores = []\n",
    "        \n",
    "        for i, (doc, emb) in enumerate(zip(candidates, candidate_embs)):\n",
    "            if i in [candidates.index(s) for s in selected]:\n",
    "                mmr_scores.append(-float('inf'))\n",
    "                continue\n",
    "            \n",
    "            # Relevance to query\n",
    "            relevance = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n",
    "            \n",
    "            # Max similarity to already selected\n",
    "            if selected_embs:\n",
    "                similarities = [np.dot(emb, s_emb) / (np.linalg.norm(emb) * np.linalg.norm(s_emb)) \n",
    "                              for s_emb in selected_embs]\n",
    "                max_sim = max(similarities)\n",
    "            else:\n",
    "                max_sim = 0\n",
    "            \n",
    "            # MMR score\n",
    "            mmr = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "            mmr_scores.append(mmr)\n",
    "        \n",
    "        # Select best MMR score\n",
    "        best_idx = np.argmax(mmr_scores)\n",
    "        selected.append(candidates[best_idx])\n",
    "        selected_embs.append(candidate_embs[best_idx])\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "def retrieve_with_reranking(query: str, embed_model: SentenceTransformer, \n",
    "                           top_k: int = 3, fetch_k: int = 20) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Two-stage: retrieve with embeddings, rerank with cross-encoder.\n",
    "    \"\"\"\n",
    "    # Stage 1: Retrieve candidates\n",
    "    candidates = retrieve_vanilla(query, embed_model, top_k=fetch_k)\n",
    "    \n",
    "    if len(candidates) <= top_k:\n",
    "        return candidates[:top_k]\n",
    "    \n",
    "    # Stage 2: Rerank with cross-encoder\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    \n",
    "    pairs = [[query, doc['content']] for doc in candidates]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Sort by reranker score\n",
    "    scored_docs = [(doc, score) for doc, score in zip(candidates, scores)]\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Update scores and return top-k\n",
    "    reranked = []\n",
    "    for doc, score in scored_docs[:top_k]:\n",
    "        doc['rerank_score'] = float(score)\n",
    "        reranked.append(doc)\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "\n",
    "RETRIEVAL_STRATEGIES = {\n",
    "    'vanilla_k3': {'func': retrieve_vanilla, 'params': {'top_k': 3}, 'notes': 'Current setup'},\n",
    "    'vanilla_k5': {'func': retrieve_vanilla, 'params': {'top_k': 5}, 'notes': 'More context'},\n",
    "    'vanilla_k10': {'func': retrieve_vanilla, 'params': {'top_k': 10}, 'notes': 'Maximum context'},\n",
    "    'threshold_0.7': {'func': retrieve_with_threshold, 'params': {'top_k': 10, 'threshold': 0.7}, 'notes': 'Quality filter'},\n",
    "    'mmr_balanced': {'func': retrieve_with_mmr, 'params': {'top_k': 3, 'lambda_param': 0.5}, 'notes': 'Balance diversity'},\n",
    "    'mmr_diverse': {'func': retrieve_with_mmr, 'params': {'top_k': 3, 'lambda_param': 0.3}, 'notes': 'More diversity'},\n",
    "    'reranked_k3': {'func': retrieve_with_reranking, 'params': {'top_k': 3, 'fetch_k': 20}, 'notes': 'Two-stage rerank'},\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(RETRIEVAL_STRATEGIES)} retrieval strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Models and Prompting Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    ")\n",
    "\n",
    "LLM_MODELS = {\n",
    "    'Llama-4-Maverick': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
    "    'DeepSeek-R1': 'DeepSeek-R1',\n",
    "    'GPT-4.1': 'gpt-4.1',\n",
    "    'GPT-5-mini': 'gpt-5-mini',\n",
    "    'Claude-Sonnet-4.5': 'claude-sonnet-4-5',\n",
    "}\n",
    "\n",
    "# Prompting strategies\n",
    "PROMPTING_STRATEGIES = {\n",
    "    'baseline': \"\"\"\n",
    "Siz SOCAR-ƒ±n tarixi neft v…ô qaz s…ôn…ôdl…ôri √ºzr…ô k√∂m…ôk√ßisiniz.\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "Kontekst…ô …ôsaslanaraq cavab verin.\n",
    "\"\"\",\n",
    "    \n",
    "    'citation_focused': \"\"\"\n",
    "Siz SOCAR-ƒ±n tarixi s…ôn…ôdl…ôr √ºzr…ô m√ºt…ôx…ôssis k√∂m…ôk√ßisisiniz.\n",
    "\n",
    "√ñN∆èMLƒ∞: H…ôr bir faktƒ± m√ºtl…ôq m…ônb…ô il…ô t…ôsdiql…ôyin (PDF adƒ± v…ô s…ôhif…ô n√∂mr…ôsi).\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "Cavab ver…ôrk…ôn:\n",
    "1. D…ôqiq faktlar yazƒ±n\n",
    "2. H…ôr faktƒ± m…ônb…ô il…ô g√∂st…ôrin: (PDF: fayl_adƒ±.pdf, S…ôhif…ô: X)\n",
    "3. Kontekstd…ô olmayan m…ôlumat …ôlav…ô etm…ôyin\n",
    "\"\"\",\n",
    "    \n",
    "    'step_by_step': \"\"\"\n",
    "Siz SOCAR-ƒ±n tarixi s…ôn…ôdl…ôr √ºzr…ô analitik k√∂m…ôk√ßisisiniz.\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "Addƒ±m-addƒ±m cavab verin:\n",
    "1. ∆èvv…ôlc…ô kontekstd…ôn …ôlaq…ôli m…ôlumatlarƒ± m√º…ôyy…ônl…ô≈üdirin\n",
    "2. Bu m…ôlumatlarƒ± t…ôhlil edin\n",
    "3. N…ôtic…ôni m…ônb…ôl…ôr il…ô birlikd…ô t…ôqdim edin\n",
    "\"\"\",\n",
    "    \n",
    "    'few_shot': \"\"\"\n",
    "Siz SOCAR-ƒ±n tarixi s…ôn…ôdl…ôr √ºzr…ô m√ºt…ôx…ôssis k√∂m…ôk√ßisisiniz.\n",
    "\n",
    "N√ºmun…ô:\n",
    "Sual: \"Pal√ßƒ±q vulkanlarƒ±nƒ±n t…ôsir radiusu n…ô q…ôd…ôrdir?\"\n",
    "Cavab: \"Sah…ô m√º≈üahid…ôl…ôri v…ô modell…ô≈üdirm…ô g√∂st…ôrir ki, pal√ßƒ±q vulkanlarƒ±nƒ±n t…ôsir radiusu t…ôqrib…ôn 10 km-dir (PDF: document_06.pdf, S…ôhif…ô: 5).\"\n",
    "\n",
    "Kontekst:\n",
    "{context}\n",
    "\n",
    "Sual: {query}\n",
    "\n",
    "Yuxarƒ±dakƒ± n√ºmun…ô kimi cavab verin - d…ôqiq, qƒ±sa, m…ônb…ô il…ô.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Configured {len(LLM_MODELS)} LLM models\")\n",
    "print(f\"‚úÖ Configured {len(PROMPTING_STRATEGIES)} prompting strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(llm_model: str, query: str, documents: List[Dict], \n",
    "                   prompt_strategy: str = 'baseline',\n",
    "                   temperature: float = 0.2) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generate answer using LLM with specified prompting strategy.\n",
    "    \"\"\"\n",
    "    # Build context\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(documents, 1):\n",
    "        context_parts.append(\n",
    "            f\"S…ôn…ôd {i} (M…ônb…ô: {doc['pdf_name']}, S…ôhif…ô {doc['page_number']}):\\n{doc['content']}\"\n",
    "        )\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Get prompt template\n",
    "    prompt_template = PROMPTING_STRATEGIES[prompt_strategy]\n",
    "    prompt = prompt_template.format(context=context, query=query)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        deployment = LLM_MODELS[llm_model]\n",
    "        \n",
    "        # GPT-5 models use max_completion_tokens, others use max_tokens\n",
    "        if deployment.startswith('gpt-5'):\n",
    "            response = azure_client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_completion_tokens=1000\n",
    "            )\n",
    "        else:\n",
    "            response = azure_client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        return answer, elapsed\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\", 0.0\n",
    "\n",
    "print(\"‚úÖ LLM generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_answer_quality(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Accuracy metrics.\"\"\"\n",
    "    ref_norm = normalize_text(reference)\n",
    "    hyp_norm = normalize_text(hypothesis)\n",
    "    \n",
    "    cer_score = cer(ref_norm, hyp_norm) * 100\n",
    "    wer_score = wer(ref_norm, hyp_norm) * 100\n",
    "    similarity = max(0, 100 - wer_score)\n",
    "    \n",
    "    return {\n",
    "        'Accuracy_Score': round(similarity, 2)\n",
    "    }\n",
    "\n",
    "def evaluate_citation_quality(answer: str, documents: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Relevance - citation quality.\"\"\"\n",
    "    pdf_names = [doc['pdf_name'].replace('.pdf', '') for doc in documents]\n",
    "    page_numbers = [str(doc['page_number']) for doc in documents]\n",
    "    \n",
    "    cited_pdfs = sum(1 for pdf in pdf_names if pdf in answer)\n",
    "    cited_pages = sum(1 for page in page_numbers if page in answer)\n",
    "    \n",
    "    citation_keywords = ['m…ônb…ô', 's…ôn…ôd', 's…ôhif…ô', 'pdf', 'document', 'page']\n",
    "    has_citation_format = any(kw in answer.lower() for kw in citation_keywords)\n",
    "    \n",
    "    citation_score = (\n",
    "        (cited_pdfs / len(pdf_names) * 40) +\n",
    "        (cited_pages / len(page_numbers) * 40) +\n",
    "        (20 if has_citation_format else 0)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Citation_Score': round(citation_score, 2),\n",
    "        'Cited_PDFs': cited_pdfs,\n",
    "        'Cited_Pages': cited_pages\n",
    "    }\n",
    "\n",
    "def evaluate_retrieval_quality(query: str, documents: List[Dict], expected_answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Measure if retrieved docs are relevant to answer.\"\"\"\n",
    "    if not documents or not expected_answer:\n",
    "        return {'Retrieval_Relevance': 0.0}\n",
    "    \n",
    "    # Simple heuristic: check if expected answer words appear in retrieved docs\n",
    "    expected_words = set(normalize_text(expected_answer).split())\n",
    "    retrieved_text = ' '.join([doc['content'] for doc in documents])\n",
    "    retrieved_words = set(normalize_text(retrieved_text).split())\n",
    "    \n",
    "    overlap = len(expected_words & retrieved_words) / len(expected_words) if expected_words else 0\n",
    "    \n",
    "    return {\n",
    "        'Retrieval_Relevance': round(overlap * 100, 2)\n",
    "    }\n",
    "\n",
    "def evaluate_completeness(answer: str) -> Dict[str, float]:\n",
    "    \"\"\"Completeness metrics.\"\"\"\n",
    "    word_count = len(answer.split())\n",
    "    \n",
    "    if word_count < 20:\n",
    "        completeness = (word_count / 20) * 100\n",
    "    elif word_count > 200:\n",
    "        completeness = 100 - ((word_count - 200) / 200 * 20)\n",
    "    else:\n",
    "        completeness = 100\n",
    "    \n",
    "    return {\n",
    "        'Completeness_Score': round(max(0, completeness), 2),\n",
    "        'Word_Count': word_count\n",
    "    }\n",
    "\n",
    "def calculate_llm_judge_score(accuracy: float, citation: float, completeness: float) -> float:\n",
    "    \"\"\"Overall LLM Judge score (weighted).\"\"\"\n",
    "    return round(\n",
    "        accuracy * 0.35 +\n",
    "        citation * 0.35 +\n",
    "        completeness * 0.30,\n",
    "        2\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Evaluation metrics ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Comprehensive Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Select what to test\n",
    "CONFIGS_TO_TEST = [\n",
    "    # Format: (embed_model, retrieval_strategy, llm_model, prompt_strategy)\n",
    "    \n",
    "    # Baseline (current setup)\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different embedding models\n",
    "    ('multilingual-e5-large', 'vanilla_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different retrieval strategies\n",
    "    ('bge-large-en', 'vanilla_k5', 'Llama-4-Maverick', 'baseline'),\n",
    "    ('bge-large-en', 'mmr_balanced', 'Llama-4-Maverick', 'baseline'),\n",
    "    ('bge-large-en', 'reranked_k3', 'Llama-4-Maverick', 'baseline'),\n",
    "    \n",
    "    # Test different LLM models\n",
    "    ('bge-large-en', 'vanilla_k3', 'GPT-5-mini', 'baseline'),\n",
    "    ('bge-large-en', 'vanilla_k3', 'Claude-Sonnet-4.5', 'baseline'),\n",
    "    \n",
    "    # Test different prompting strategies\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'citation_focused'),\n",
    "    ('bge-large-en', 'vanilla_k3', 'Llama-4-Maverick', 'few_shot'),\n",
    "    \n",
    "    # Best combinations\n",
    "    ('bge-large-en', 'reranked_k3', 'GPT-5-mini', 'citation_focused'),\n",
    "    ('bge-large-en', 'mmr_balanced', 'Claude-Sonnet-4.5', 'citation_focused'),\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(CONFIGS_TO_TEST)} configurations on {len(questions)} questions\")\n",
    "print(f\"Total API calls: ~{len(CONFIGS_TO_TEST) * len(questions)}\")\n",
    "print(\"This will take 15-30 minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for config_idx, (embed_key, retrieval_key, llm_key, prompt_key) in enumerate(CONFIGS_TO_TEST, 1):\n",
    "    config_name = f\"{embed_key}_{retrieval_key}_{llm_key}_{prompt_key}\"\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Config {config_idx}/{len(CONFIGS_TO_TEST)}: {config_name}\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Get components\n",
    "    embed_model = embedding_cache[embed_key]\n",
    "    retrieval_func = RETRIEVAL_STRATEGIES[retrieval_key]['func']\n",
    "    retrieval_params = RETRIEVAL_STRATEGIES[retrieval_key]['params']\n",
    "    \n",
    "    config_results = []\n",
    "    \n",
    "    for example_key, messages in questions.items():\n",
    "        user_msg = [m for m in messages if m['role'] == 'user'][-1]\n",
    "        query = user_msg['content']\n",
    "        \n",
    "        print(f\"\\n  {example_key}: {query[:60]}...\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        documents = retrieval_func(query, embed_model, **retrieval_params)\n",
    "        print(f\"    Retrieved {len(documents)} docs\")\n",
    "        \n",
    "        # Generate answer\n",
    "        answer, response_time = generate_answer(llm_key, query, documents, prompt_key)\n",
    "        \n",
    "        if answer.startswith('ERROR'):\n",
    "            print(f\"    ‚ùå {answer}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    ‚úÖ Generated in {response_time:.2f}s\")\n",
    "        \n",
    "        # Evaluate\n",
    "        expected = expected_answers.get(example_key, {}).get('Answer', '')\n",
    "        \n",
    "        accuracy_metrics = calculate_answer_quality(expected, answer) if expected else {'Accuracy_Score': 0}\n",
    "        citation_metrics = evaluate_citation_quality(answer, documents)\n",
    "        retrieval_metrics = evaluate_retrieval_quality(query, documents, expected)\n",
    "        completeness_metrics = evaluate_completeness(answer)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        llm_judge_score = calculate_llm_judge_score(\n",
    "            accuracy_metrics['Accuracy_Score'],\n",
    "            citation_metrics['Citation_Score'],\n",
    "            completeness_metrics['Completeness_Score']\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'Config': config_name,\n",
    "            'Embedding_Model': embed_key,\n",
    "            'Retrieval_Strategy': retrieval_key,\n",
    "            'LLM_Model': llm_key,\n",
    "            'Prompt_Strategy': prompt_key,\n",
    "            'Question': example_key,\n",
    "            'Query': query[:80],\n",
    "            'Num_Docs_Retrieved': len(documents),\n",
    "            'Response_Time': round(response_time, 2),\n",
    "            'LLM_Judge_Score': llm_judge_score,\n",
    "            **accuracy_metrics,\n",
    "            **citation_metrics,\n",
    "            **retrieval_metrics,\n",
    "            **completeness_metrics,\n",
    "            'Answer_Preview': answer[:150]\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        config_results.append(result)\n",
    "    \n",
    "    # Show config summary\n",
    "    if config_results:\n",
    "        avg_score = sum(r['LLM_Judge_Score'] for r in config_results) / len(config_results)\n",
    "        avg_time = sum(r['Response_Time'] for r in config_results) / len(config_results)\n",
    "        print(f\"\\n  üìä Config Summary:\")\n",
    "        print(f\"     Avg LLM Judge Score: {avg_score:.2f}%\")\n",
    "        print(f\"     Avg Response Time: {avg_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"‚úÖ Comprehensive benchmark complete!\")\n",
    "print(f\"{'='*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Aggregate by configuration\n",
    "config_summary = df.groupby('Config').agg({\n",
    "    'LLM_Judge_Score': 'mean',\n",
    "    'Accuracy_Score': 'mean',\n",
    "    'Citation_Score': 'mean',\n",
    "    'Retrieval_Relevance': 'mean',\n",
    "    'Completeness_Score': 'mean',\n",
    "    'Response_Time': 'mean',\n",
    "    'Embedding_Model': 'first',\n",
    "    'Retrieval_Strategy': 'first',\n",
    "    'LLM_Model': 'first',\n",
    "    'Prompt_Strategy': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# Sort by LLM Judge Score\n",
    "config_summary = config_summary.sort_values('LLM_Judge_Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"üìä CONFIGURATION RANKINGS (By LLM Judge Score)\")\n",
    "print(\"=\"*120)\n",
    "display_cols = ['Embedding_Model', 'Retrieval_Strategy', 'LLM_Model', 'Prompt_Strategy', \n",
    "                'LLM_Judge_Score', 'Accuracy_Score', 'Citation_Score', 'Response_Time']\n",
    "print(config_summary[display_cols].to_string())\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze impact of each component\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üîç COMPONENT IMPACT ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Embedding Models\n",
    "print(\"\\nüìö EMBEDDING MODELS:\")\n",
    "embed_impact = df.groupby('Embedding_Model')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for model, score in embed_impact.items():\n",
    "    print(f\"   {model}: {score:.2f}%\")\n",
    "\n",
    "# 2. Retrieval Strategies\n",
    "print(\"\\nüîé RETRIEVAL STRATEGIES:\")\n",
    "retrieval_impact = df.groupby('Retrieval_Strategy')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for strategy, score in retrieval_impact.items():\n",
    "    notes = RETRIEVAL_STRATEGIES[strategy]['notes']\n",
    "    print(f\"   {strategy}: {score:.2f}% ({notes})\")\n",
    "\n",
    "# 3. LLM Models\n",
    "print(\"\\nü§ñ LLM MODELS:\")\n",
    "llm_impact = df.groupby('LLM_Model')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for model, score in llm_impact.items():\n",
    "    print(f\"   {model}: {score:.2f}%\")\n",
    "\n",
    "# 4. Prompting Strategies\n",
    "print(\"\\nüí¨ PROMPTING STRATEGIES:\")\n",
    "prompt_impact = df.groupby('Prompt_Strategy')['LLM_Judge_Score'].mean().sort_values(ascending=False)\n",
    "for strategy, score in prompt_impact.items():\n",
    "    print(f\"   {strategy}: {score:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory - using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'rag_optimization_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Top Configurations\n",
    "ax1 = axes[0, 0]\n",
    "top_configs = config_summary.head(10)\n",
    "config_labels = [c.split('_')[-2] + '+' + c.split('_')[-1] for c in top_configs.index]\n",
    "ax1.barh(config_labels, top_configs['LLM_Judge_Score'], color=sns.color_palette('viridis', len(top_configs)))\n",
    "ax1.set_xlabel('LLM Judge Score (%)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Top 10 Configurations', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlim(0, 100)\n",
    "for i, score in enumerate(top_configs['LLM_Judge_Score']):\n",
    "    ax1.text(score + 1, i, f'{score:.1f}', va='center', fontsize=10)\n",
    "\n",
    "# 2. Embedding Model Impact\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(embed_impact.index, embed_impact.values, color='skyblue', alpha=0.8)\n",
    "ax2.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Embedding Model Impact', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "for i, (model, score) in enumerate(embed_impact.items()):\n",
    "    ax2.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "# 3. Retrieval Strategy Impact\n",
    "ax3 = axes[0, 2]\n",
    "ax3.bar(retrieval_impact.index, retrieval_impact.values, color='coral', alpha=0.8)\n",
    "ax3.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Retrieval Strategy Impact', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "for i, (strategy, score) in enumerate(retrieval_impact.items()):\n",
    "    ax3.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=9)\n",
    "\n",
    "# 4. LLM Model Impact\n",
    "ax4 = axes[1, 0]\n",
    "ax4.bar(llm_impact.index, llm_impact.values, color='mediumseagreen', alpha=0.8)\n",
    "ax4.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('LLM Model Impact', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylim(0, 100)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "for i, (model, score) in enumerate(llm_impact.items()):\n",
    "    ax4.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "# 5. Prompting Strategy Impact\n",
    "ax5 = axes[1, 1]\n",
    "ax5.bar(prompt_impact.index, prompt_impact.values, color='mediumpurple', alpha=0.8)\n",
    "ax5.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Prompting Strategy Impact', fontsize=13, fontweight='bold')\n",
    "ax5.set_ylim(0, 100)\n",
    "ax5.tick_params(axis='x', rotation=45)\n",
    "for i, (strategy, score) in enumerate(prompt_impact.items()):\n",
    "    ax5.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n",
    "\n",
    "# 6. Score Components (best config)\n",
    "ax6 = axes[1, 2]\n",
    "best_config = config_summary.iloc[0]\n",
    "components = ['Accuracy', 'Citation', 'Completeness']\n",
    "scores = [best_config['Accuracy_Score'], best_config['Citation_Score'], best_config['Completeness_Score']]\n",
    "colors_comp = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = ax6.bar(components, scores, color=colors_comp, alpha=0.8)\n",
    "ax6.set_ylabel('Score (%)', fontsize=11, fontweight='bold')\n",
    "ax6.set_title(f'Best Config Components\\n{best_config.name.split(\"_\")[2]}', fontsize=13, fontweight='bold')\n",
    "ax6.set_ylim(0, 100)\n",
    "for i, score in enumerate(scores):\n",
    "    ax6.text(i, score + 2, f'{score:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to '{output_dir}/results.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# Create output directory - using dynamic path\noutput_dir = OUTPUT_DIR / 'rag_optimization_benchmark'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. Top Configurations\nax1 = axes[0, 0]\ntop_configs = config_summary.head(10)\nconfig_labels = [c.split('_')[-2] + '+' + c.split('_')[-1] for c in top_configs.index]\nax1.barh(config_labels, top_configs['LLM_Judge_Score'], color=sns.color_palette('viridis', len(top_configs)))\nax1.set_xlabel('LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax1.set_title('Top 10 Configurations', fontsize=13, fontweight='bold')\nax1.set_xlim(0, 100)\nfor i, score in enumerate(top_configs['LLM_Judge_Score']):\n    ax1.text(score + 1, i, f'{score:.1f}', va='center', fontsize=10)\n\n# 2. Embedding Model Impact\nax2 = axes[0, 1]\nax2.bar(embed_impact.index, embed_impact.values, color='skyblue', alpha=0.8)\nax2.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax2.set_title('Embedding Model Impact', fontsize=13, fontweight='bold')\nax2.set_ylim(0, 100)\nax2.tick_params(axis='x', rotation=45)\nfor i, (model, score) in enumerate(embed_impact.items()):\n    ax2.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 3. Retrieval Strategy Impact\nax3 = axes[0, 2]\nax3.bar(retrieval_impact.index, retrieval_impact.values, color='coral', alpha=0.8)\nax3.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax3.set_title('Retrieval Strategy Impact', fontsize=13, fontweight='bold')\nax3.set_ylim(0, 100)\nax3.tick_params(axis='x', rotation=45)\nfor i, (strategy, score) in enumerate(retrieval_impact.items()):\n    ax3.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=9)\n\n# 4. LLM Model Impact\nax4 = axes[1, 0]\nax4.bar(llm_impact.index, llm_impact.values, color='mediumseagreen', alpha=0.8)\nax4.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax4.set_title('LLM Model Impact', fontsize=13, fontweight='bold')\nax4.set_ylim(0, 100)\nax4.tick_params(axis='x', rotation=45)\nfor i, (model, score) in enumerate(llm_impact.items()):\n    ax4.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 5. Prompting Strategy Impact\nax5 = axes[1, 1]\nax5.bar(prompt_impact.index, prompt_impact.values, color='mediumpurple', alpha=0.8)\nax5.set_ylabel('Avg LLM Judge Score (%)', fontsize=11, fontweight='bold')\nax5.set_title('Prompting Strategy Impact', fontsize=13, fontweight='bold')\nax5.set_ylim(0, 100)\nax5.tick_params(axis='x', rotation=45)\nfor i, (strategy, score) in enumerate(prompt_impact.items()):\n    ax5.text(i, score + 2, f'{score:.1f}', ha='center', fontsize=10)\n\n# 6. Score Components (best config)\nax6 = axes[1, 2]\nbest_config = config_summary.iloc[0]\ncomponents = ['Accuracy', 'Citation', 'Completeness']\nscores = [best_config['Accuracy_Score'], best_config['Citation_Score'], best_config['Completeness_Score']]\ncolors_comp = ['#FF6B6B', '#4ECDC4', '#45B7D1']\nbars = ax6.bar(components, scores, color=colors_comp, alpha=0.8)\nax6.set_ylabel('Score (%)', fontsize=11, fontweight='bold')\nax6.set_title(f'Best Config Components\\n{best_config.name.split(\"_\")[2]}', fontsize=13, fontweight='bold')\nax6.set_ylim(0, 100)\nfor i, score in enumerate(scores):\n    ax6.text(i, score + 2, f'{score:.1f}%', ha='center', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n‚úÖ Visualization saved to '{output_dir}/results.png'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config = config_summary.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ OPTIMAL RAG CONFIGURATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n‚úÖ Best Configuration: {best_config.name}\")\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"   LLM Judge Score: {best_config['LLM_Judge_Score']:.2f}%\")\n",
    "print(f\"   Accuracy: {best_config['Accuracy_Score']:.2f}%\")\n",
    "print(f\"   Citation Quality: {best_config['Citation_Score']:.2f}%\")\n",
    "print(f\"   Completeness: {best_config['Completeness_Score']:.2f}%\")\n",
    "print(f\"   Avg Response Time: {best_config['Response_Time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Components:\")\n",
    "print(f\"   Embedding Model: {best_config['Embedding_Model']}\")\n",
    "print(f\"      ‚Üí {EMBEDDING_MODELS[best_config['Embedding_Model']]['name']}\")\n",
    "print(f\"   Retrieval Strategy: {best_config['Retrieval_Strategy']}\")\n",
    "print(f\"      ‚Üí {RETRIEVAL_STRATEGIES[best_config['Retrieval_Strategy']]['notes']}\")\n",
    "print(f\"   LLM Model: {best_config['LLM_Model']}\")\n",
    "print(f\"   Prompting Strategy: {best_config['Prompt_Strategy']}\")\n",
    "\n",
    "print(f\"\\nüí° Key Findings:\")\n",
    "print(f\"   1. Best Embedding: {embed_impact.index[0]} ({embed_impact.values[0]:.2f}%)\")\n",
    "print(f\"   2. Best Retrieval: {retrieval_impact.index[0]} ({retrieval_impact.values[0]:.2f}%)\")\n",
    "print(f\"   3. Best LLM: {llm_impact.index[0]} ({llm_impact.values[0]:.2f}%)\")\n",
    "print(f\"   4. Best Prompt: {prompt_impact.index[0]} ({prompt_impact.values[0]:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ Hackathon Impact:\")\n",
    "print(f\"   LLM Quality = 30% of total score\")\n",
    "print(f\"   Your score: {best_config['LLM_Judge_Score']:.2f}% √ó 30% = {best_config['LLM_Judge_Score'] * 0.3:.2f} points\")\n",
    "\n",
    "baseline = df[df['Config'].str.contains('baseline')].iloc[0] if len(df[df['Config'].str.contains('baseline')]) > 0 else None\n",
    "if baseline is not None:\n",
    "    improvement = best_config['LLM_Judge_Score'] - baseline['LLM_Judge_Score']\n",
    "    print(f\"\\nüìà Improvement vs Baseline:\")\n",
    "    print(f\"   +{improvement:.2f}% quality improvement\")\n",
    "    print(f\"   = +{improvement * 0.3:.2f} hackathon points\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìù IMPLEMENTATION CHECKLIST\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\n1. Use embedding model: {EMBEDDING_MODELS[best_config['Embedding_Model']]['name']}\")\n",
    "print(f\"2. Implement retrieval: {best_config['Retrieval_Strategy']}\")\n",
    "print(f\"3. Use LLM model: {best_config['LLM_Model']}\")\n",
    "print(f\"4. Apply prompt: {best_config['Prompt_Strategy']}\")\n",
    "print(f\"\\n5. Expected performance:\")\n",
    "print(f\"   - LLM Judge Score: {best_config['LLM_Judge_Score']:.2f}%\")\n",
    "print(f\"   - Response time: ~{best_config['Response_Time']:.1f}s\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results\n",
    "from pathlib import Path\n",
    "\n",
    "# Using dynamic path\n",
    "output_dir = OUTPUT_DIR / 'rag_optimization_benchmark'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "config_summary.to_csv(output_dir / 'summary.csv', encoding='utf-8')\n",
    "\n",
    "# Save component impacts\n",
    "impacts = pd.DataFrame({\n",
    "    'Embedding_Impact': embed_impact,\n",
    "    'Retrieval_Impact': retrieval_impact.reindex(embed_impact.index, fill_value=0),\n",
    "    'LLM_Impact': llm_impact.reindex(embed_impact.index, fill_value=0),\n",
    "    'Prompt_Impact': prompt_impact.reindex(embed_impact.index, fill_value=0)\n",
    "}).fillna(0)\n",
    "impacts.to_csv(output_dir / 'component_impacts.csv', encoding='utf-8')\n",
    "\n",
    "print(\"\\n‚úÖ Results exported to output/rag_optimization_benchmark/:\")\n",
    "print(\"   - detailed_results.csv (all tests)\")\n",
    "print(\"   - summary.csv (config rankings)\")\n",
    "print(\"   - component_impacts.csv (component analysis)\")\n",
    "print(\"   - results.png (visualizations)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}