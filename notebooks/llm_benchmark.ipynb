{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Benchmarking for SOCAR Hackathon RAG Chatbot\n",
    "\n",
    "This notebook tests different LLM models for the `/llm` endpoint to find the best performer.\n",
    "\n",
    "## Evaluation Criteria (LLM Judge Metrics):\n",
    "- **Accuracy**: Is the answer correct?\n",
    "- **Relevance**: Are retrieved citations relevant?\n",
    "- **Completeness**: Does it fully answer the question?\n",
    "- **Citation Quality**: Proper sources with page numbers?\n",
    "- **Response Time**: Speed of generation\n",
    "\n",
    "## Available LLM Models:\n",
    "1. **Llama-4-Maverick-17B-128E-Instruct-FP8** (Current choice, open-source)\n",
    "2. **DeepSeek-R1** (Open-source reasoning model)\n",
    "3. **GPT-4.1** (Strong general performance)\n",
    "4. **GPT-5, GPT-5-mini**\n",
    "5. **Claude Sonnet 4.5** (Best quality)\n",
    "6. **Claude Opus 4.1**\n",
    "7. **Phi-4-multimodal-instruct**\n",
    "8. **gpt-oss-120b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai pinecone-client sentence-transformers python-dotenv pandas matplotlib seaborn jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/SOCAR_Hackathon/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import AzureOpenAI\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Questions and Expected Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 test cases\n",
      "\n",
      "Test Questions:\n",
      "1. Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasiyasƒ± t…ôl…ôb olunur?...\n",
      "2. Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq edilmi≈üdir v…ô bunun m...\n",
      "3. Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO oksidl…ôri arasƒ±nda ha...\n",
      "4. Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin…ô …ôsas…ôn neft v…ô qaz...\n",
      "5. Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n"
     ]
    }
   ],
   "source": [
    "# Load sample questions\n",
    "with open('docs/sample_questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "# Load expected answers\n",
    "with open('docs/sample_answers.json', 'r', encoding='utf-8') as f:\n",
    "    expected_answers = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(questions)} test cases\")\n",
    "print(\"\\nTest Questions:\")\n",
    "for i, (key, msgs) in enumerate(questions.items(), 1):\n",
    "    user_msg = [m for m in msgs if m['role'] == 'user'][-1]\n",
    "    print(f\"{i}. {key}: {user_msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Vector Database and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector DB connected: {'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '188',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Sun, 14 Dec 2025 03:21:33 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '4',\n",
      "                                    'x-pinecone-request-id': '3979707437017514155',\n",
      "                                    'x-pinecone-request-latency-ms': '4'}},\n",
      " 'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 1300}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 1300,\n",
      " 'vector_type': 'dense'}\n",
      "‚úÖ Embedding model loaded: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))\n",
    "index = pc.Index(os.getenv('PINECONE_INDEX_NAME', 'hackathon'))\n",
    "\n",
    "# Initialize embedding model (same as used for ingestion)\n",
    "embed_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "\n",
    "print(f\"‚úÖ Vector DB connected: {index.describe_index_stats()}\")\n",
    "print(f\"‚úÖ Embedding model loaded: {embed_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Retrieved 3 documents for test query\n",
      "Top result: document_10.pdf, page 8 (score: 0.767)\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from vector database.\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    # Search vector DB\n",
    "    results = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Extract documents\n",
    "    documents = []\n",
    "    for match in results['matches']:\n",
    "        documents.append({\n",
    "            'pdf_name': match['metadata'].get('pdf_name', 'unknown.pdf'),\n",
    "            'page_number': match['metadata'].get('page_number', 0),\n",
    "            'content': match['metadata'].get('text', ''),\n",
    "            'score': match.get('score', 0.0)\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"Pal√ßƒ±q vulkanlarƒ±nƒ±n t…ôsir radiusu n…ô q…ôd…ôrdir?\"\n",
    "test_docs = retrieve_documents(test_query)\n",
    "print(f\"\\n‚úÖ Retrieved {len(test_docs)} documents for test query\")\n",
    "print(f\"Top result: {test_docs[0]['pdf_name']}, page {test_docs[0]['page_number']} (score: {test_docs[0]['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM Client Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Azure OpenAI\nazure_client = AzureOpenAI(\n    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-08-01-preview'),\n    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT')\n)\n\nLLM_MODELS = {\n    'Llama-4-Maverick': 'Llama-4-Maverick-17B-128E-Instruct-FP8',\n    'DeepSeek-R1': 'DeepSeek-R1',\n    'GPT-4.1': 'gpt-4.1',\n    'GPT-5-mini': 'gpt-5-mini',\n    'Claude-Sonnet-4.5': 'claude-sonnet-4-5',\n}\n\ndef generate_answer(model_name: str, query: str, documents: List[Dict], \n                   temperature: float = 0.2, max_tokens: int = 1000) -> Tuple[str, float]:\n    \"\"\"\n    Generate answer using specified LLM model.\n    Returns: (answer, response_time)\n    \"\"\"\n    # Build context from retrieved documents\n    context_parts = []\n    for i, doc in enumerate(documents, 1):\n        context_parts.append(\n            f\"Document {i} (Source: {doc['pdf_name']}, Page {doc['page_number']}):\\n{doc['content']}\"\n        )\n    context = \"\\n\\n\".join(context_parts)\n    \n    # Create prompt\n    prompt = f\"\"\"Siz SOCAR-ƒ±n tarixi neft v…ô qaz s…ôn…ôdl…ôri √ºzr…ô m√ºt…ôx…ôssis k√∂m…ôk√ßisisiniz.\n\nKontekst (…ôlaq…ôli s…ôn…ôdl…ôr):\n{context}\n\nSual: {query}\n\n∆ètraflƒ± cavab verin v…ô m√ºtl…ôq s…ôn…ôd m…ônb…ôl…ôrin…ô istinad edin (PDF adƒ± v…ô s…ôhif…ô n√∂mr…ôsi il…ô).\nCavabƒ±nƒ±z d…ôqiq, faktlara …ôsaslanan v…ô kontekst m…ôlumatlarƒ±ndan istifad…ô ed…ôn olmalƒ±dƒ±r.\"\"\"\n    \n    # Get model deployment\n    deployment = MODELS[model_name]['deployment']\n    \n    try:\n        start_time = time.time()\n        \n        # GPT-5 models use max_completion_tokens, others use max_tokens\n        if deployment.startswith('gpt-5'):\n            response = azure_client.chat.completions.create(\n                model=deployment,\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=temperature,\n                max_completion_tokens=max_tokens\n            )\n        else:\n            response = azure_client.chat.completions.create(\n                model=deployment,\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=temperature,\n                max_tokens=max_tokens\n            )\n        \n        response_time = time.time() - start_time\n        answer = response.choices[0].message.content\n        \n        return answer, response_time\n    \n    except Exception as e:\n        return f\"ERROR: {str(e)}\", 0.0\n\nprint(f\"\\n‚úÖ Configured {len(LLM_MODELS)} LLM models for testing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions ready\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text for comparison.\"\"\"\n",
    "    import re\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def calculate_answer_similarity(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate similarity between generated and expected answer.\n",
    "    Lower is better for error rates.\n",
    "    \"\"\"\n",
    "    ref_norm = normalize_text(reference)\n",
    "    hyp_norm = normalize_text(hypothesis)\n",
    "    \n",
    "    # Character Error Rate\n",
    "    cer_score = cer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Word Error Rate  \n",
    "    wer_score = wer(ref_norm, hyp_norm) * 100\n",
    "    \n",
    "    # Similarity scores (higher is better)\n",
    "    similarity = max(0, 100 - wer_score)\n",
    "    \n",
    "    return {\n",
    "        'CER': round(cer_score, 2),\n",
    "        'WER': round(wer_score, 2),\n",
    "        'Similarity': round(similarity, 2)\n",
    "    }\n",
    "\n",
    "def check_citations(answer: str, documents: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Check if answer includes proper citations.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Check for PDF names\n",
    "    pdf_names = [doc['pdf_name'] for doc in documents]\n",
    "    cited_pdfs = sum(1 for pdf in pdf_names if pdf.replace('.pdf', '') in answer)\n",
    "    \n",
    "    # Check for page numbers\n",
    "    page_numbers = [str(doc['page_number']) for doc in documents]\n",
    "    cited_pages = sum(1 for page in page_numbers if page in answer)\n",
    "    \n",
    "    # Check for source keywords\n",
    "    source_keywords = ['m…ônb…ô', 's…ôn…ôd', 's…ôhif…ô', 'pdf', 'document', 'page', 'source']\n",
    "    has_source_ref = any(kw in answer.lower() for kw in source_keywords)\n",
    "    \n",
    "    citation_score = (\n",
    "        (cited_pdfs / len(pdf_names) * 40) +  # 40% for PDF citation\n",
    "        (cited_pages / len(page_numbers) * 40) +  # 40% for page citation\n",
    "        (20 if has_source_ref else 0)  # 20% for having source keywords\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Citation_Score': round(citation_score, 2),\n",
    "        'Cited_PDFs': cited_pdfs,\n",
    "        'Cited_Pages': cited_pages,\n",
    "        'Has_Source_Reference': has_source_ref\n",
    "    }\n",
    "\n",
    "def evaluate_completeness(answer: str, min_length: int = 100) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Evaluate answer completeness.\n",
    "    \"\"\"\n",
    "    word_count = len(answer.split())\n",
    "    char_count = len(answer)\n",
    "    \n",
    "    # Penalize very short or very long answers\n",
    "    if char_count < min_length:\n",
    "        completeness_score = (char_count / min_length) * 100\n",
    "    elif char_count > 2000:\n",
    "        completeness_score = 100 - ((char_count - 2000) / 2000 * 20)  # Penalty for verbosity\n",
    "    else:\n",
    "        completeness_score = 100\n",
    "    \n",
    "    return {\n",
    "        'Completeness_Score': round(max(0, completeness_score), 2),\n",
    "        'Word_Count': word_count,\n",
    "        'Char_Count': char_count\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Benchmark on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 5 models on 5 questions...\n",
      "\n",
      "This may take several minutes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select models to test (you can comment out models to skip)\n",
    "MODELS_TO_TEST = [\n",
    "    'Llama-4-Maverick-17B',\n",
    "    'DeepSeek-R1',\n",
    "    'GPT-4.1',\n",
    "    'GPT-5-mini',\n",
    "    'Claude-Sonnet-4.5',\n",
    "    # 'Claude-Opus-4.1',  # Uncomment to test\n",
    "    # 'Phi-4-multimodal',  # Uncomment to test\n",
    "    # 'GPT-OSS-120B',  # Uncomment to test\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {len(questions)} questions...\\n\")\n",
    "print(\"This may take several minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing: Llama-4-Maverick-17B\n",
      "================================================================================\n",
      "\n",
      "  Question Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasi...\n",
      "  ‚úÖ Response time: 4.39s\n",
      "\n",
      "  Question Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq e...\n",
      "  ‚úÖ Response time: 3.74s\n",
      "\n",
      "  Question Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO o...\n",
      "  ‚úÖ Response time: 4.07s\n",
      "\n",
      "  Question Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin...\n",
      "  ‚úÖ Response time: 4.20s\n",
      "\n",
      "  Question Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n",
      "  ‚úÖ Response time: 3.50s\n",
      "\n",
      "  üìä Llama-4-Maverick-17B Summary:\n",
      "     Avg Response Time: 3.98s\n",
      "     Avg Similarity: 0.0%\n",
      "     Avg Citation Score: 84.0%\n",
      "     Avg Completeness: 100.0%\n",
      "\n",
      "================================================================================\n",
      "Testing: DeepSeek-R1\n",
      "================================================================================\n",
      "\n",
      "  Question Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasi...\n",
      "  ‚úÖ Response time: 10.00s\n",
      "\n",
      "  Question Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq e...\n",
      "  ‚úÖ Response time: 10.39s\n",
      "\n",
      "  Question Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO o...\n",
      "  ‚úÖ Response time: 10.73s\n",
      "\n",
      "  Question Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin...\n",
      "  ‚úÖ Response time: 12.17s\n",
      "\n",
      "  Question Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n",
      "  ‚úÖ Response time: 10.72s\n",
      "\n",
      "  üìä DeepSeek-R1 Summary:\n",
      "     Avg Response Time: 10.80s\n",
      "     Avg Similarity: 0.0%\n",
      "     Avg Citation Score: 80.0%\n",
      "     Avg Completeness: 67.7%\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-4.1\n",
      "================================================================================\n",
      "\n",
      "  Question Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasi...\n",
      "  ‚úÖ Response time: 6.66s\n",
      "\n",
      "  Question Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq e...\n",
      "  ‚úÖ Response time: 5.05s\n",
      "\n",
      "  Question Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO o...\n",
      "  ‚úÖ Response time: 7.65s\n",
      "\n",
      "  Question Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin...\n",
      "  ‚úÖ Response time: 6.68s\n",
      "\n",
      "  Question Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n",
      "  ‚úÖ Response time: 3.69s\n",
      "\n",
      "  üìä GPT-4.1 Summary:\n",
      "     Avg Response Time: 5.95s\n",
      "     Avg Similarity: 0.0%\n",
      "     Avg Citation Score: 84.0%\n",
      "     Avg Completeness: 93.5%\n",
      "\n",
      "================================================================================\n",
      "Testing: GPT-5-mini\n",
      "================================================================================\n",
      "\n",
      "  Question Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasi...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Question Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq e...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Question Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO o...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Question Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "  Question Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.\", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}\n",
      "\n",
      "================================================================================\n",
      "Testing: Claude-Sonnet-4.5\n",
      "================================================================================\n",
      "\n",
      "  Question Example1: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasi...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Question Example2: Q…ôrbi Ab≈üeron yataƒüƒ±nda suvurma t…ôdbirl…ôri hansƒ± tarixd…ô v…ô hansƒ± layda t…ôtbiq e...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Question Example3: Pirallahƒ± strukturunda 1253 n√∂mr…ôli quyudan g√∂t√ºr√ºlm√º≈ü n√ºmun…ôl…ôrd…ô SiO2 v…ô CaO o...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Question Example4: Bakƒ± arxipelaqƒ± (BA) v…ô A≈üaƒüƒ± K√ºr √ß√∂k…ôkliyi (AK√á) √º√ß√ºn geotemperatur x…ôrit…ôl…ôrin...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "  Question Example5: Bu zonada hansƒ± prosesl…ôr ba≈ü verir?...\n",
      "  ‚ùå Failed: ERROR: Error code: 400 - {'error': {'code': 'unknown_model', 'message': 'Unknown model: claude-sonnet-4-5', 'details': 'Unknown model: claude-sonnet-4-5'}}\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Benchmarking complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Run benchmark\n",
    "results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for example_key, messages in questions.items():\n",
    "        # Get the last user message (the actual question)\n",
    "        user_msg = [m for m in messages if m['role'] == 'user'][-1]\n",
    "        query = user_msg['content']\n",
    "        \n",
    "        print(f\"\\n  Question {example_key}: {query[:80]}...\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        documents = retrieve_documents(query, top_k=3)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer, response_time = generate_answer(model_name, query, documents)\n",
    "        \n",
    "        if answer.startswith('ERROR'):\n",
    "            print(f\"  ‚ùå Failed: {answer}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  ‚úÖ Response time: {response_time:.2f}s\")\n",
    "        \n",
    "        # Get expected answer\n",
    "        expected = expected_answers.get(example_key, {}).get('Answer', '')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        similarity_metrics = calculate_answer_similarity(expected, answer) if expected else {'CER': 0, 'WER': 0, 'Similarity': 0}\n",
    "        citation_metrics = check_citations(answer, documents)\n",
    "        completeness_metrics = evaluate_completeness(answer)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            'Model': model_name,\n",
    "            'Question': example_key,\n",
    "            'Query': query[:100],\n",
    "            'Answer': answer[:200] + '...',\n",
    "            'Response_Time': round(response_time, 2),\n",
    "            **similarity_metrics,\n",
    "            **citation_metrics,\n",
    "            **completeness_metrics,\n",
    "            'Open_Source': MODELS[model_name]['open_source'],\n",
    "            'Architecture_Score': MODELS[model_name]['architecture_score']\n",
    "        }\n",
    "        \n",
    "        model_results.append(result)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Show summary for this model\n",
    "    if model_results:\n",
    "        avg_response_time = sum(r['Response_Time'] for r in model_results) / len(model_results)\n",
    "        avg_similarity = sum(r['Similarity'] for r in model_results) / len(model_results)\n",
    "        avg_citation = sum(r['Citation_Score'] for r in model_results) / len(model_results)\n",
    "        avg_completeness = sum(r['Completeness_Score'] for r in model_results) / len(model_results)\n",
    "        \n",
    "        print(f\"\\n  üìä {model_name} Summary:\")\n",
    "        print(f\"     Avg Response Time: {avg_response_time:.2f}s\")\n",
    "        print(f\"     Avg Similarity: {avg_similarity:.1f}%\")\n",
    "        print(f\"     Avg Citation Score: {avg_citation:.1f}%\")\n",
    "        print(f\"     Avg Completeness: {avg_completeness:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úÖ Benchmarking complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregate Results and Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìä LLM BENCHMARKING RESULTS - MODEL SUMMARY\n",
      "====================================================================================================\n",
      "                      Response_Time  Similarity  Citation_Score  Completeness_Score     CER     WER  Open_Source Architecture_Score  Quality_Score\n",
      "Model                                                                                                                                             \n",
      "Llama-4-Maverick-17B           3.98         0.0            84.0              100.00  330.97  378.42         True               High          59.40\n",
      "GPT-4.1                        5.95         0.0            84.0               93.54  755.19  780.64        False             Medium          57.46\n",
      "DeepSeek-R1                   10.80         0.0            80.0               67.73  855.43  992.02         True               High          48.32\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate aggregate scores per model\n",
    "model_summary = df.groupby('Model').agg({\n",
    "    'Response_Time': 'mean',\n",
    "    'Similarity': 'mean',\n",
    "    'Citation_Score': 'mean',\n",
    "    'Completeness_Score': 'mean',\n",
    "    'CER': 'mean',\n",
    "    'WER': 'mean',\n",
    "    'Open_Source': 'first',\n",
    "    'Architecture_Score': 'first'\n",
    "}).round(2)\n",
    "\n",
    "# Calculate overall quality score (weighted average)\n",
    "model_summary['Quality_Score'] = (\n",
    "    model_summary['Similarity'] * 0.35 +  # 35% answer accuracy\n",
    "    model_summary['Citation_Score'] * 0.35 +  # 35% citation quality\n",
    "    model_summary['Completeness_Score'] * 0.30  # 30% completeness\n",
    ").round(2)\n",
    "\n",
    "# Sort by Quality Score\n",
    "model_summary = model_summary.sort_values('Quality_Score', ascending=False)\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìä LLM BENCHMARKING RESULTS - MODEL SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(model_summary.to_string())\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comprehensive visualization\nimport os\nfrom pathlib import Path\n\n# Create output directory\noutput_dir = Path('output/llm_benchmark')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\nmodels = model_summary.index.tolist()\ncolors = sns.color_palette('husl', len(models))\n\n# 1. Overall Quality Score\nax1 = axes[0, 0]\nbars1 = ax1.barh(models, model_summary['Quality_Score'], color=colors)\nax1.set_xlabel('Quality Score (Higher is Better)', fontsize=11)\nax1.set_title('Overall Quality Score\\n(Similarity 35% + Citation 35% + Completeness 30%)', \n              fontsize=12, fontweight='bold')\nax1.set_xlim(0, 100)\nfor i, (model, score) in enumerate(zip(models, model_summary['Quality_Score'])):\n    ax1.text(score + 1, i, f'{score:.1f}', va='center', fontsize=10, fontweight='bold')\n\n# 2. Answer Similarity (Accuracy)\nax2 = axes[0, 1]\nax2.barh(models, model_summary['Similarity'], color=colors)\nax2.set_xlabel('Similarity to Expected Answer (%)', fontsize=11)\nax2.set_title('Answer Accuracy', fontsize=12, fontweight='bold')\nax2.set_xlim(0, 100)\nfor i, (model, score) in enumerate(zip(models, model_summary['Similarity'])):\n    ax2.text(score + 1, i, f'{score:.1f}%', va='center', fontsize=9)\n\n# 3. Citation Quality\nax3 = axes[0, 2]\nax3.barh(models, model_summary['Citation_Score'], color=colors)\nax3.set_xlabel('Citation Score (%)', fontsize=11)\nax3.set_title('Citation Quality\\n(PDF names + Page numbers)', fontsize=12, fontweight='bold')\nax3.set_xlim(0, 100)\nfor i, (model, score) in enumerate(zip(models, model_summary['Citation_Score'])):\n    ax3.text(score + 1, i, f'{score:.1f}%', va='center', fontsize=9)\n\n# 4. Response Time\nax4 = axes[1, 0]\nax4.barh(models, model_summary['Response_Time'], color=colors)\nax4.set_xlabel('Response Time (seconds - Lower is Better)', fontsize=11)\nax4.set_title('Speed Performance', fontsize=12, fontweight='bold')\nfor i, (model, time) in enumerate(zip(models, model_summary['Response_Time'])):\n    ax4.text(time + 0.1, i, f'{time:.2f}s', va='center', fontsize=9)\n\n# 5. Completeness\nax5 = axes[1, 1]\nax5.barh(models, model_summary['Completeness_Score'], color=colors)\nax5.set_xlabel('Completeness Score (%)', fontsize=11)\nax5.set_title('Answer Completeness', fontsize=12, fontweight='bold')\nax5.set_xlim(0, 100)\nfor i, (model, score) in enumerate(zip(models, model_summary['Completeness_Score'])):\n    ax5.text(score + 1, i, f'{score:.1f}%', va='center', fontsize=9)\n\n# 6. Error Rates (CER vs WER)\nax6 = axes[1, 2]\nx = range(len(models))\nwidth = 0.35\nax6.bar([i - width/2 for i in x], model_summary['CER'], width, label='CER', alpha=0.8)\nax6.bar([i + width/2 for i in x], model_summary['WER'], width, label='WER', alpha=0.8)\nax6.set_ylabel('Error Rate (% - Lower is Better)', fontsize=11)\nax6.set_title('Error Rates', fontsize=12, fontweight='bold')\nax6.set_xticks(x)\nax6.set_xticklabels(models, rotation=45, ha='right')\nax6.legend()\nax6.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(output_dir / 'results.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\n‚úÖ Visualization saved to '{output_dir}/results.png'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Rankings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üèÜ FINAL RANKINGS\n",
      "====================================================================================================\n",
      "                      Rank  Quality_Score  Similarity  Citation_Score  Completeness_Score  Response_Time  Open_Source Architecture_Score\n",
      "Model                                                                                                                                   \n",
      "Llama-4-Maverick-17B     1          59.40         0.0            84.0              100.00           3.98         True               High\n",
      "GPT-4.1                  2          57.46         0.0            84.0               93.54           5.95        False             Medium\n",
      "DeepSeek-R1              3          48.32         0.0            80.0               67.73          10.80         True               High\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "üí° RECOMMENDATIONS FOR HACKATHON\n",
      "====================================================================================================\n",
      "\n",
      "ü•á Best Overall Quality: Llama-4-Maverick-17B\n",
      "   Quality Score: 59.4%\n",
      "   Similarity: 0.0%\n",
      "   Citation Score: 84.0%\n",
      "   Response Time: 3.98s\n",
      "   Open Source: True\n",
      "   Architecture Score: High\n",
      "\n",
      "üîì Best Open-Source Model: Llama-4-Maverick-17B\n",
      "   Quality Score: 59.4%\n",
      "   Architecture Score: High (Better for hackathon!)\n",
      "   Response Time: 3.98s\n",
      "\n",
      "‚ö° Fastest Model: Llama-4-Maverick-17B\n",
      "   Response Time: 3.98s\n",
      "   Quality Score: 59.4%\n",
      "\n",
      "====================================================================================================\n",
      "üìù FINAL RECOMMENDATION\n",
      "====================================================================================================\n",
      "\n",
      "Scoring Breakdown:\n",
      "  - LLM Quality: 30% of total hackathon score\n",
      "  - Architecture: 20% of total hackathon score (open-source preferred!)\n",
      "\n",
      "Best Choice:\n",
      "  ‚úÖ Llama-4-Maverick-17B - Best balance of quality and architecture score\n",
      "     Only 0.0% quality drop for higher architecture score!\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create rankings table\n",
    "rankings = model_summary[[\n",
    "    'Quality_Score', 'Similarity', 'Citation_Score', 'Completeness_Score', \n",
    "    'Response_Time', 'Open_Source', 'Architecture_Score'\n",
    "]].copy()\n",
    "\n",
    "rankings.insert(0, 'Rank', range(1, len(rankings) + 1))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üèÜ FINAL RANKINGS\")\n",
    "print(\"=\"*100)\n",
    "print(rankings.to_string())\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Winner analysis\n",
    "best_overall = rankings.index[0]\n",
    "best_open_source = rankings[rankings['Open_Source'] == True].index[0] if any(rankings['Open_Source']) else None\n",
    "fastest = model_summary['Response_Time'].idxmin()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üí° RECOMMENDATIONS FOR HACKATHON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nü•á Best Overall Quality: {best_overall}\")\n",
    "print(f\"   Quality Score: {model_summary.loc[best_overall, 'Quality_Score']:.1f}%\")\n",
    "print(f\"   Similarity: {model_summary.loc[best_overall, 'Similarity']:.1f}%\")\n",
    "print(f\"   Citation Score: {model_summary.loc[best_overall, 'Citation_Score']:.1f}%\")\n",
    "print(f\"   Response Time: {model_summary.loc[best_overall, 'Response_Time']:.2f}s\")\n",
    "print(f\"   Open Source: {model_summary.loc[best_overall, 'Open_Source']}\")\n",
    "print(f\"   Architecture Score: {model_summary.loc[best_overall, 'Architecture_Score']}\")\n",
    "\n",
    "if best_open_source:\n",
    "    print(f\"\\nüîì Best Open-Source Model: {best_open_source}\")\n",
    "    print(f\"   Quality Score: {model_summary.loc[best_open_source, 'Quality_Score']:.1f}%\")\n",
    "    print(f\"   Architecture Score: {model_summary.loc[best_open_source, 'Architecture_Score']} (Better for hackathon!)\")\n",
    "    print(f\"   Response Time: {model_summary.loc[best_open_source, 'Response_Time']:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚ö° Fastest Model: {fastest}\")\n",
    "print(f\"   Response Time: {model_summary.loc[fastest, 'Response_Time']:.2f}s\")\n",
    "print(f\"   Quality Score: {model_summary.loc[fastest, 'Quality_Score']:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üìù FINAL RECOMMENDATION\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nScoring Breakdown:\")\n",
    "print(\"  - LLM Quality: 30% of total hackathon score\")\n",
    "print(\"  - Architecture: 20% of total hackathon score (open-source preferred!)\")\n",
    "print(\"\\nBest Choice:\")\n",
    "if best_open_source and model_summary.loc[best_open_source, 'Quality_Score'] >= model_summary.loc[best_overall, 'Quality_Score'] * 0.9:\n",
    "    print(f\"  ‚úÖ {best_open_source} - Best balance of quality and architecture score\")\n",
    "    print(f\"     Only {model_summary.loc[best_overall, 'Quality_Score'] - model_summary.loc[best_open_source, 'Quality_Score']:.1f}% quality drop for higher architecture score!\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ {best_overall} - Highest quality, use if quality gap is significant\")\n",
    "    if best_open_source:\n",
    "        print(f\"  ‚ö†Ô∏è  Consider {best_open_source} for higher architecture score (trade-off: {model_summary.loc[best_overall, 'Quality_Score'] - model_summary.loc[best_open_source, 'Quality_Score']:.1f}% quality)\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nfrom pathlib import Path\n\noutput_dir = Path('output/llm_benchmark')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\ndf.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\nmodel_summary.to_csv(output_dir / 'summary.csv', encoding='utf-8')\nrankings.to_csv(output_dir / 'rankings.csv', index=False, encoding='utf-8')\n\nprint(\"\\n‚úÖ Results exported to output/llm_benchmark/:\")\nprint(\"   - detailed_results.csv (all questions and answers)\")\nprint(\"   - summary.csv (model averages)\")\nprint(\"   - rankings.csv (final rankings)\")\nprint(\"   - results.png (visualizations)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Answer Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "üìù SAMPLE ANSWER COMPARISON - Example1\n",
      "====================================================================================================\n",
      "\n",
      "‚ùì Question: Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn hansƒ± …ôsas amill…ôrin inteqrasiyasƒ± t…ôl…ôb olunur?\n",
      "\n",
      "‚úÖ Expected Answer:\n",
      "Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn d√ºzg√ºn se√ßilmi≈ü texnoloji inteqrasiya (horizontal v…ô √ßoxt…ôr…ôfli qazma texnikalarƒ±) v…ô qazma m…ôhlullarƒ±nƒ±n s…ôm…ôr…ôli idar…ô edilm…ôsi t…ôl…ôb olunur. Bu yana≈üma h…ôm iqtisadi, h…ôm d…ô ekoloji baxƒ±mdan √ºst√ºnl√ºk yaradƒ±r.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ü§ñ Llama-4-Maverick-17B (Quality: 59.4%, Time: 4.39s):\n",
      "Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn d√ºzg√ºn se√ßilmi≈ü texnoloji inteqrasiya v…ô qazma m…ôhlullarƒ±nƒ±n s…ôm…ôr…ôli idar…ôsi …ôsas amill…ôrdir. Bu, Document 1 (document_11.pdf, S…ôhif…ô 3)-d…ô qeyd olun...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ü§ñ DeepSeek-R1 (Quality: 48.3%, Time: 10.00s):\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking about the main factors that need to be integrated to achieve more production with fewer wells. They provided three documents, so I need to ...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "ü§ñ GPT-4.1 (Quality: 57.5%, Time: 6.66s):\n",
      "Daha az quyu il…ô daha √ßox hasilat …ôld…ô etm…ôk √º√ß√ºn bir ne√ß…ô …ôsas amilin inteqrasiyasƒ± t…ôl…ôb olunur. Bu amill…ôr a≈üaƒüƒ±dakƒ± kimi sistematik ≈ü…ôkild…ô s…ôn…ôd m…ônb…ôl…ôrin…ô istinadla izah olunur:\n",
      "\n",
      "1. **D√ºzg√ºn se...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show sample answers for first question\n",
    "sample_question = 'Example1'\n",
    "sample_results = df[df['Question'] == sample_question]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"üìù SAMPLE ANSWER COMPARISON - {sample_question}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n‚ùì Question: {questions[sample_question][0]['content']}\")\n",
    "print(f\"\\n‚úÖ Expected Answer:\\n{expected_answers[sample_question]['Answer']}\")\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "\n",
    "for _, row in sample_results.iterrows():\n",
    "    print(f\"\\nü§ñ {row['Model']} (Quality: {model_summary.loc[row['Model'], 'Quality_Score']:.1f}%, Time: {row['Response_Time']:.2f}s):\")\n",
    "    print(f\"{row['Answer']}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}